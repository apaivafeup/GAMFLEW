TY  - CONF
TI  - Virtualized-Fault Injection Testing: A Machine Learning Approach
T2  - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)
SP  - 297
EP  - 308
AU  - H. Khosrowjerdi
AU  - K. Meinke
AU  - A. Rasmusson
PY  - 2018
DO  - 10.1109/ICST.2018.00037
JO  - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)
Y1  - 9-13 April 2018
AB  - We introduce a new methodology for virtualized fault injection testing of safety critical embedded systems. This approach fully automates the key steps of test case generation, fault injection and verdict construction. We use machine learning to reverse engineer models of the system under test. We use model checking to generate test verdicts with respect to safety requirements formalised in temporal logic. We exemplify our approach by implementing a tool chain based on integrating the QEMU hardware emulator, the GNU debugger GDB and the LBTest requirements testing tool. This tool chain is then evaluated on two industrial safety critical applications from the automotive sector.
ER  - 

TY  - JOUR
TI  - ARTDL: Adaptive Random Testing for Deep Learning Systems
T2  - IEEE Access
SP  - 3055
EP  - 3064
AU  - M. Yan
AU  - L. Wang
AU  - A. Fei
PY  - 2020
DO  - 10.1109/ACCESS.2019.2962695
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 8
VL  - 8
JA  - IEEE Access
Y1  - 2020
AB  - With recent breakthroughs in Deep Learning (DL), DL systems are increasingly deployed in safety-critical fields. Hence, some software testing methods are required to ensure the reliability and safety of DL systems. Since the rules of DL systems are inferred from training data, it is difficult to know the implementation rules about each behavior of DL systems. At the same time, Random Testing (RT) is a popular testing method and the knowledge about software implementation is not needed when we use RT. Therefore, RT is very suitable for the testing of DL systems. And the existing mechanisms for testing DL systems also depend heavily on RT by the labeled test data. In order to increase the effectiveness of RT for DL systems, we design, implement and evaluate the Adaptive Random Testing for DL systems (ARTDL), which is the first Adaptive Random Testing (ART) method to improve the effectiveness of RT for DL systems. ARTDL refers to the idea of ART. That is, fewer test cases are needed to detect failures by selecting the test case with the furthest distance from non-failure-causing test cases. Firstly, we propose the Feature-based Euclidean Distance (FED) as the distance metric that can be used to measure the difference between failure-causing inputs and non-failure-causing inputs. Secondly, we verify the availability of FED by presenting the failure pattern of DL models. Finally, we design ARTDL algorithm to generate the test cases that are more likely to cause failures based on the FED. We implement ARTDL to test top performing DL models in the field of image classification and automatic driving. The results show that, on average, the number of test cases used to find the first bug is reduced by 62.74% through ARTDL, compared with RT.
ER  - 

TY  - CONF
TI  - How do Students Test Software Units?
T2  - 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
SP  - 189
EP  - 198
AU  - L. Bijlsma
AU  - N. Doorn
AU  - H. Passier
AU  - H. Pootjes
AU  - S. Stuurman
PY  - 2021
DO  - 10.1109/ICSE-SEET52601.2021.00029
JO  - 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
Y1  - 25-28 May 2021
AB  - We gained insight into ideas and beliefs on testing of students who finished an introductory course on programming without any formal education on testing. We asked students to fill in a small survey, to do four exercises and to fill in a second survey. We interviewed eleven of these students in semi-structured interviews, to obtain more in-depth insight. The main outcome is that students do not test systematically, while most of them think they do test systematically. One of the misconceptions we found is that most students can only think of test cases based on programming code. Even if no code was provided (black-box testing), students try to come up with code to base their test cases on.
ER  - 

TY  - CONF
TI  - Debugging Effectiveness of LBT: An Empirical Study
T2  - 2022 17th International Conference on Emerging Technologies (ICET)
SP  - 136
EP  - 141
AU  - W. A. Khan
AU  - M. A. Sindhu
PY  - 2022
DO  - 10.1109/ICET56601.2022.10004661
JO  - 2022 17th International Conference on Emerging Technologies (ICET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 17th International Conference on Emerging Technologies (ICET)
Y1  - 29-30 Nov. 2022
AB  - Debugging is an essential step in the software development life-cycle. Automatic test generation techniques and tools have significantly improved software development. Test generation tools are evaluated based on their ability to reveal faults or coverage capability. The Learning-based Testing (LBT) is a black-box testing tool that uses a learning algorithm to generate tests. This tool has not been explored concerning debugging effectiveness. We conducted a study involving 20 human subjects to evaluate LBT and its effectiveness in debugging. We used two case studies of reactive systems and two open-source model-based testing tools to evaluate the effectiveness of LBT over other tools. We have discovered that LBT is an effective tool for automatic test case generation and show how LBT may improve the debugging process. The study found that LBT can aid less experienced and experienced developers with debugging. The effectiveness of LBT is 95%, and the efficiency is higher than other tools. Besides, we discover that skilled developers have performed satisfactorily on other tools, yet there is a noticeable difference in time.
ER  - 

TY  - CONF
TI  - Supervised Learning for Test Suit Selection in Continuous Integration
T2  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 239
EP  - 246
AU  - R. Martins
AU  - R. Abreu
AU  - M. Lopes
AU  - J. Nadkarni
PY  - 2021
DO  - 10.1109/ICSTW52544.2021.00048
JO  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 12-16 April 2021
AB  - Continuous Integration is the process of merging code changes into a software project. Keeping the master branch always updated and unfailingly is very computationally expensive due to the number of tests and code that needs to be executed. The waiting times also increase the time required for debugging. This paper proposes a solution to reduce the execution time of the testing phase, by selecting only a subset of all the tests, given some code changes. This is accomplished by training a Machine Learning (ML) Classifier with features such as code/test files history fails, extension code files that tend to generate more errors during the testing phase, and others. The results obtained by the best ML classifier showed results comparable with the recent literature done in the same area. This model managed to reduce the median test execution time by nearly 10 minutes while maintaining 97% of recall. Additionally, the impact of innocent commits and flaky tests was taken into account and studied to understand a particular industrial context.
ER  - 

TY  - CONF
TI  - Using Machine Learning to Prioritize Automated Testing in an Agile Environment
T2  - 2019 Conference on Information Communications Technology and Society (ICTAS)
SP  - 1
EP  - 6
AU  - L. Butgereit
PY  - 2019
DO  - 10.1109/ICTAS.2019.8703639
JO  - 2019 Conference on Information Communications Technology and Society (ICTAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 Conference on Information Communications Technology and Society (ICTAS)
Y1  - 6-8 March 2019
AB  - Automated software testing is an integral part of most Agile methodologies. In the case of the Scrum Agile methodology, the definition of done includes the completion of tests. As a software project matures, however, the number of tests increases to such a point that the time required to run all the tests often hinders the speed in which artifacts can be deployed. This paper describes a technique of using machine learning to help prioritize automated testing to ensure that tests which have a higher probability of failing are executed early in the test run giving the programmers an early indication of problems. In order to do this, various metrics are collected about the software under test including Cyclomatic values, Halstead-based values, and Chidamber-Kemere values. In addition, the historical commit messages from the source code control system is accessed to see if there had been defects in the various source classes previously. From these two inputs, a data file can be created which contains various metrics and whether or not there had been defects in these source files previously. This data file can then be sent to Weka to create a decision tree indicating which measurements indicate potential defects. The model created by Weka can then then be used in future to attempt to predict where defects might be in the source files and then prioritize testing appropriately.
ER  - 

TY  - CONF
TI  - Automated Web Testing using Machine Learning and Containerization
T2  - 2022 26th International Conference on Circuits, Systems, Communications and Computers (CSCC)
SP  - 113
EP  - 121
AU  - O. Loubiri
AU  - S. Maag
PY  - 2022
DO  - 10.1109/CSCC55931.2022.00029
JO  - 2022 26th International Conference on Circuits, Systems, Communications and Computers (CSCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 26th International Conference on Circuits, Systems, Communications and Computers (CSCC)
Y1  - 19-22 July 2022
AB  - Testing practices in software engineering constantly evolves due to the complexity of the systems. This has opened the space for new testing methods to try to integrate artificial intelligence with software testing tools. Automation testing refers to the use of strategies and tools which reduce the need for manual or human involvement in redundant and repetitive tasks that tend to cause human errors and then generating and executing automatically test cases. However, while a test scripts is generated, its reuse may be challenging for several reasons. In our Web systems context, a web page may be modified leading to the adaptation of the testing architecture and the eventual rewriting of the test scripts. It becomes time and effort consuming to create generic test cases that can be applied on any website. Websites keep on changing dynamically and the testers need to adapt to these changes each time and alter the test cases. These changes are often made manually or using external scripts. In this paper, we propose an approach allowing the test scripts to automatically adapt to these eventual changes of the web pages by using containers and a learning technique. We defined and implemented an algorithm on a well designed test framework and successfully evaluated our approach on thousands of websites.
ER  - 

TY  - CONF
TI  - An agile learning design method for open educational resources
T2  - 2015 IEEE Frontiers in Education Conference (FIE)
SP  - 1
EP  - 9
AU  - M. M. Arimoto
AU  - E. F. Barbosa
AU  - L. Barroca
PY  - 2015
DO  - 10.1109/FIE.2015.7344334
JO  - 2015 IEEE Frontiers in Education Conference (FIE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 IEEE Frontiers in Education Conference (FIE)
Y1  - 21-24 Oct. 2015
AB  - Open Educational Resources (OERs) have provided new perspectives for the construction, access and sharing of knowledge. While OERs can bring benefits to, and impact on education, there are still challenges to their widespread production and use. One of the challenges faced by developers (including educators and practitioners) of OERs has been how to produce quality and relevant learning materials, capable of being reused and adapted in different learning situations. In our work we propose and define an agile learning design method to support the design and creation of OERs. It is based on agile practices from software engineering and on practices of learning design from the OULDI project at the UK Open University. We illustrate our ideas with an experiment that validates the proposed method through its application in the design and creation of an OER in the software testing domain. The results obtained so far have shown that the method is feasible and effective for the design and creation of OERs.
ER  - 

TY  - CONF
TI  - Model-Based Testing IoT Communication via Active Automata Learning
T2  - 2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)
SP  - 276
EP  - 287
AU  - M. Tappler
AU  - B. K. Aichernig
AU  - R. Bloem
PY  - 2017
DO  - 10.1109/ICST.2017.32
JO  - 2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)
Y1  - 13-17 March 2017
AB  - This paper presents a learning-based approach to detecting failures in reactive systems. The technique is based on inferring models of multiple implementations of a common specification which are pair-wise cross-checked for equivalence. Any counterexample to equivalence is flagged as suspicious and has to be analysed manually. Hence, it is possible to find possible failures in a semi-automatic way without prior modelling. We show that the approach is effective by means of a case study. For this case study, we carried out experiments in which we learned models of five implementations of MQTT brokers/servers, a protocol used in the Internet of Things. Examining these models, we found several violations of the MQTT specification. All but one of the considered implementations showed faulty behaviour. In the analysis, we discuss effectiveness and also issues we faced.
ER  - 

TY  - CONF
TI  - DeepBoundary: A Coverage Testing Method of Deep Learning Software based on Decision Boundary Representation
T2  - 2022 IEEE 22nd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)
SP  - 166
EP  - 172
AU  - Y. Liu
AU  - L. Feng
AU  - X. Wang
AU  - S. Zhang
PY  - 2022
DO  - 10.1109/QRS-C57518.2022.00032
JO  - 2022 IEEE 22nd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)
IS  - 
SN  - 2693-9371
VO  - 
VL  - 
JA  - 2022 IEEE 22nd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)
Y1  - 5-9 Dec. 2022
AB  - With the increasing application of Deep Learning (DL) Software in safety-critical fields such as autonomous driving, we need adequate testing to ensure software quality. Observing the decision-making behavior of a Deep Neural Network (DNN) is an essential step in DL software testing. Taking Guiding Deep Learning System Testing Using Surprise Adequacy (SADL) as an example, it uses the independent neuron activation values in the DNN to represent the decision-making behavior. However, the behavior of the DNN needs to be jointly determined by the continuous outputs of all neurons. As a result, the coverage value of SADL constant volatility and lack of stability. To mitigate this problem, we propose a coverage testing method based on the decision boundary representation, DeepBoundary, for the decision-making behavior of DL software. Unlike SADL, DeepBoundary generates decision boundary data to represent the decision behavior of the DNN, which makes the testing results more stable. On this basis, we calculate the kernel density between the testing data and the decision boundary data. It measures the position of the testing data in the decision space and the distance from the decision boundary. Finally, as an adequacy indicator, we calculate the decision boundary density coverage (DBC) of the entire testing set. The experiment on the dataset MNIST and two DL software shows that DeepBoundary can generate actual decision boundary data. The average confidence error in the DNNs output layer is only 4.20E-05. Compared with SADL, DeepBoundary has a stronger correlation with the defect detection ratio, which can more accurately represent testing adequacy.
ER  - 

TY  - CONF
TI  - Efficient state synchronisation in model-based testing through reinforcement learning
T2  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
SP  - 368
EP  - 380
AU  - U. C. Türker
AU  - R. M. Hierons
AU  - M. R. Mousavi
AU  - I. Y. Tyukin
PY  - 2021
DO  - 10.1109/ASE51524.2021.9678566
JO  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
IS  - 
SN  - 2643-1572
VO  - 
VL  - 
JA  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
Y1  - 15-19 Nov. 2021
AB  - Model-based testing is a structured method to test complex systems. Scaling up model-based testing to large systems requires improving the efficiency of various steps involved in testcase generation and more importantly, in test-execution. One of the most costly steps of model-based testing is to bring the system to a known state, best achieved through synchronising sequences. A synchronising sequence is an input sequence that brings a given system to a predetermined state regardless of system’s initial state. Depending on the structure, the system might be complete, i.e., all inputs are applicable at every state of the system. However, some systems are partial and in this case not all inputs are usable at every state. Derivation of synchronising sequences from complete or partial systems is a challenging task. In this paper, we introduce a novel Q-learning algorithm that can derive synchronising sequences from systems with complete or partial structures. The proposed algorithm is faster and can process larger systems than the fastest sequential algorithm that derives synchronising sequences from complete systems. Moreover, the proposed method is also faster and can process larger systems than the most recent massively parallel algorithm that derives synchronising sequences from partial systems. Furthermore, the proposed algorithm generates shorter synchronising sequences.
ER  - 

TY  - JOUR
TI  - Collaborative assessments in computer science education: A survey
T2  - Tsinghua Science and Technology
SP  - 435
EP  - 445
AU  - H. Yuan
AU  - P. Cao
PY  - 2019
DO  - 10.26599/TST.2018.9010108
JO  - Tsinghua Science and Technology
IS  - 4
SN  - 1007-0214
VO  - 24
VL  - 24
JA  - Tsinghua Science and Technology
Y1  - Aug. 2019
AB  - As computer science enrollments continue to surge, assessments that involve student collaboration may play a more critical role in improving student learning. We provide a review on some of the most commonly adopted collaborative assessments in computer science, including pair programming, collaborative exams, and group projects. Existing research on these assessment formats is categorized and compared. We also discuss potential future research topics on the aforementioned collaborative assessment formats.
ER  - 

TY  - CONF
TI  - On Learning Meaningful Assert Statements for Unit Test Cases
T2  - 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
SP  - 1398
EP  - 1409
AU  - C. Watson
AU  - M. Tufano
AU  - K. Moran
AU  - G. Bavota
AU  - D. Poshyvanyk
PY  - 2020
DO  - 
JO  - 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
IS  - 
SN  - 1558-1225
VO  - 
VL  - 
JA  - 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
Y1  - 5-11 Oct. 2020
AB  - Software testing is an essential part of the software lifecycle and requires a substantial amount of time and effort. It has been estimated that software developers spend close to 50% of their time on testing the code they write. For these reasons, a long standing goal within the research community is to (partially) automate software testing. While several techniques and tools have been proposed to automatically generate test methods, recent work has criticized the quality and usefulness of the assert statements they generate. Therefore, we employ a Neural Machine Translation (NMT) based approach called Atlas (AuTomatic Learning of Assert Statements) to automatically generate meaningful assert statements for test methods. Given a test method and a focal method (i.e., the main method under test), Atlas can predict a meaningful assert statement to assess the correctness of the focal method. We applied Atlas to thousands of test methods from GitHub projects and it was able to predict the exact assert statement manually written by developers in 31% of the cases when only considering the top-1 predicted assert. When considering the top-5 predicted assert statements, Atlas is able to predict exact matches in 50% of the cases. These promising results hint to the potential usefulness of our approach as (i) a complement to automatic test case generation techniques, and (ii) a code completion support for developers, who can benefit from the recommended assert statements while writing test code.
ER  - 

TY  - CONF
TI  - Combination Frequency Differencing for Identifying Design Weaknesses in Physical Unclonable Functions
T2  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 110
EP  - 117
AU  - D. R. Kuhn
AU  - M. S. Raunak
AU  - C. Prado
AU  - V. C. Patil
AU  - R. N. Kacker
PY  - 2022
DO  - 10.1109/ICSTW55395.2022.00032
JO  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 4-13 April 2022
AB  - Combinatorial coverage measures have been defined and applied to a wide range of problems. These methods have been developed using measures that depend on the inclusion or absence of t-tuples of values in inputs and test cases. We extend these coverage measures to include the frequency of occurrence of combinations, in an approach that we refer to as combination frequency differencing (CFD). This method is particularly suited to artificial intelligence and machine learning (AI/ML) applications, where training data sets used in learning systems are dependent on the prevalence of various attributes of elements of class and non-class sets. We illustrate the use of this method by applying it to analyzing the susceptibility of physical unclonable functions (PUFs) to machine learning attacks. Preliminary results suggest that the method may be useful for identifying bit combinations that have a disproportionately strong influence on PUF response bit values.
ER  - 

TY  - CONF
TI  - Automated Testing of Software that Uses Machine Learning APIs
T2  - 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
SP  - 212
EP  - 224
AU  - C. Wan
AU  - S. Liu
AU  - S. Xie
AU  - Y. Liu
AU  - H. Hoffmann
AU  - M. Maire
AU  - S. Lu
PY  - 2022
DO  - 10.1145/3510003.3510068
JO  - 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
IS  - 
SN  - 1558-1225
VO  - 
VL  - 
JA  - 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Y1  - 25-27 May 2022
AB  - An increasing number of software applications incorporate machine learning (ML) solutions for cognitive tasks that statistically mimic human behaviors. To test such software, tremendous human effort is needed to design image/text/audio inputs that are relevant to the software, and to judge whether the software is processing these inputs as most human beings do. Even when misbehavior is exposed, it is often unclear whether the culprit is inside the cognitive ML API or the code using the API. This paper presents Keeper, a new testing tool for software that uses cognitive ML APIs. Keeper designs a pseudo-inverse function for each ML API that reverses the corresponding cognitive task in an empirical way (e.g., an image search engine pseudo-reverses the image-classification API), and incorporates these pseudo-inverse functions into a symbolic execution engine to automatically gener-ate relevant image/text/audio inputs and judge output correctness. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used in software to alleviate the misbehavior. Our evalu-ation on a variety of open-source applications shows that Keeper greatly improves the branch coverage, while identifying many pre-viously unknown bugs.
ER  - 

TY  - CONF
TI  - Risk Assessment and Management using Machine Learning Approaches
T2  - 2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
SP  - 663
EP  - 667
AU  - S. Darandale
AU  - R. Mehta
PY  - 2022
DO  - 10.1109/ICAAIC53929.2022.9792870
JO  - 2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
Y1  - 9-11 May 2022
AB  - Appropriate estimation of risk and its management is a significant part in the lifecycle of software engineering. Risks are the crucial factors that affect the project's growth and work. The unidentified risk may damage the crucial components of the software. If the risk is underestimated, it may compromise the development and testing of the software. Looking at the facts, risk management stands as the crucial phase of any software development lifecycle. Risk management and assessment include various activities like recognizing, analyzing, planning, and controlling events that threaten project development. Machine learning, a booming area of research, has also shown its dominance in the field of risk assessment, however, it is still in the preliminary phase and has limited applicability. This paper considers various machine learning classifiers, such as Naive Bayesian, Decision Tree, Artificial Neural Network, and K Nearest Neighbor, and shows their comparative analysis and usage in developing the software risk assessment estimator. In order to boost the usage of machine learning models in the field of risk assessment, based on the findings, future directions are provided for researchers and stakeholders.
ER  - 

TY  - CONF
TI  - Semantic-based and Learning-based Regression Test Selection focusing on Test Objectives
T2  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 281
EP  - 287
AU  - J. Suzuki
AU  - Y. Nishi
AU  - S. Tanaka
AU  - K. Naruse
AU  - M. Shimoji
AU  - Z. Zhong
PY  - 2023
DO  - 10.1109/ICSTW58534.2023.00057
JO  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 16-20 April 2023
AB  - Machine learning-based regression test selection (MLRTS) has typically arisen in major cloud companies with full CI/CD pipelines. Various approaches for RTS are researched except for deep semantics of test cases, i. e. test objectives. Test objectives are essential but implicit information hard to calculate. In this paper, we propose a test architecture of a test objective-based MLRTS (TOMLRTS). First, word vectors are converted from explicitly written words in all regression test cases by Word2Vec. Second, semantic test objective clusters are formed by K-means++ to express all test objectives in the regression test cases. Third, distance vectors are constituted, whose elements are distances from each test case to the test objective clusters. Priorities of regression test cases are then calculated by MLRTS additionally according to the distance vectors. We additionally evaluate TOMLRTS compared to Facebook's MLRTS for commercial software. TOMLRTS selected test cases to detect bugs more rapidly.
ER  - 

TY  - CONF
TI  - QBE: QLearning-Based Exploration of Android Applications
T2  - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)
SP  - 105
EP  - 115
AU  - Y. Koroglu
AU  - A. Sen
AU  - O. Muslu
AU  - Y. Mete
AU  - C. Ulker
AU  - T. Tanriverdi
AU  - Y. Donmez
PY  - 2018
DO  - 10.1109/ICST.2018.00020
JO  - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)
Y1  - 9-13 April 2018
AB  - Android applications are used extensively around the world. Many of these applications contain potential crashes. Black-box testing of Android applications has been studied over the last decade to detect these crashes. In this paper, we propose QLearning-Based Exploration (QBE), a fully automated black-box testing methodology, which explores GUI actions using a well-known reinforcement learning technique called QLearning. QBE performs automata learning to obtain a model of the AUT, and generates replayable test suites. Specifically, QBE learns from a set of existing applications the kinds of actions that are most useful in order to reach a particular objective such as detecting crashes or increasing activity coverage. To the best of our knowledge, ours is the first machine learning based approach in Android GUI Testing. We conduct experiments on a test set of 100 AUTs obtained from the commonly used F-Droid benchmarks to show the effectiveness of QBE. We show that QBE performs better than all compared black-box tools in terms of activity coverage and number of distinct detected crashes. We make QBE and our experimental data available online.
ER  - 

TY  - CONF
TI  - Software Defect Prediction based on Bayesian Optimization Random Forest
T2  - 2022 9th International Conference on Dependable Systems and Their Applications (DSA)
SP  - 1012
EP  - 1013
AU  - Y. Shen
AU  - S. Hu
AU  - S. Cai
AU  - M. Chen
PY  - 2022
DO  - 10.1109/DSA56465.2022.00149
JO  - 2022 9th International Conference on Dependable Systems and Their Applications (DSA)
IS  - 
SN  - 2767-6684
VO  - 
VL  - 
JA  - 2022 9th International Conference on Dependable Systems and Their Applications (DSA)
Y1  - 4-5 Aug. 2022
AB  - Software defect prediction is an important way to make rational use of software testing data resources and improve software performance. However, people have used a variety of machine learning algorithms to establish defect prediction models, their parameter selection is still a problem. To solve this problem, the software defect prediction method based on Bayesian optimization random forest is proposed. This method preproccess data firstly, and then the Bayesian optimization algorithm is used to tune the hyperparameters of the random forest model. Finally the NASA MDP datasets are used for simulation verification. The experimental results show that our model has better performance for software defect prediction.
ER  - 

TY  - CONF
TI  - Software Defect Data Collection Framework for Github
T2  - 2022 12th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
SP  - 82
EP  - 87
AU  - V. Suhag
AU  - S. K. Dubey
AU  - B. K. Sharma
PY  - 2022
DO  - 10.1109/Confluence52989.2022.9734131
JO  - 2022 12th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 12th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
Y1  - 27-28 Jan. 2022
AB  - Software has become part of every sphere of life. This increasing dependence on software has put tremendous pressure on software development teams to deliver software applications as early as possible at the cost of compromised software quality and reliability. Software quality requires extensive testing and validation of software, which is not possible with limited human resources, time and budget, so researchers moved to a new paradigm of software quality assurance i.e., Software Defect Prediction (SDP). SDP aims to build automated Machine Learning (ML) models to aid development teams in prioritizing the key aspects of software testing while maintaining the short software development life cycle. SDP requires huge amount of data to train and test ML models, traditionally PROMISE and NASA defect datasets are most prominently used by researchers, but with changes in programming languages, programming styles and limited size of datasets has made them infeasible for SDP in current scenarios. In this paper, we have developed a software defect dataset collection framework, which mines commit level defect data from GitHub. The efficiency of data mining, accuracy of data and validity of data is verified by SDP models. Results shows that proposed method is feasible as well as efficient to execute even on regular computer systems.
ER  - 

TY  - CONF
TI  - Using Developer Factors and Horizontal Partitioning to Recommend Bug Severity in Open-Source Software Projects
T2  - 2022 17th International Conference on Emerging Technologies (ICET)
SP  - 130
EP  - 135
AU  - H. M. W. Amjad
AU  - Z. A. Rana
PY  - 2022
DO  - 10.1109/ICET56601.2022.10004678
JO  - 2022 17th International Conference on Emerging Technologies (ICET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 17th International Conference on Emerging Technologies (ICET)
Y1  - 29-30 Nov. 2022
AB  - Software testing is performed during engineering of software to uncover the faults hidden in software systems. This data about the faults and failures is recorded in problem report forms. In addition to recording the symptoms of the problems, the problem report forms record severity of the fault. This severity of the potential failure refers to the impact on user or business. Usually, this severity is assigned by humans responsible for testing of software. Given the fact that faults in the software are result of human errors, literature has reported the use of developers' information to predict faults in software. Using the same intuition, this paper investigates the use of developers' information (along with the static code metrics) to determine the extent of high severity faults in software. To this end, three open-source software projects are studied in this paper and the code files are identified that have a chance of high severity faults. This recommendation can help testing teams identify fault prone files and label code files with high, medium, or low chance of severe faults. Static code metrics for the three projects are collected through SonarQube and developer related factors are extracted from the GitHub through custom scripts. This data is used to develop machine learning classifiers and code files are labelled as high, medium, and low number of severe faults. In order to further study the relationship between the number of developers and the files with severe faults, association between the developer factors and the presence of high severity faults is found. Study of this association reveals that if there are more developers working on a file, the file is more probable to have critical faults.
ER  - 

TY  - CONF
TI  - AI for Testing Today and Tomorrow: Industry Perspectives
T2  - 2019 IEEE International Conference On Artificial Intelligence Testing (AITest)
SP  - 81
EP  - 88
AU  - T. M. King
AU  - J. Arbon
AU  - D. Santiago
AU  - D. Adamo
AU  - W. Chin
AU  - R. Shanmugam
PY  - 2019
DO  - 10.1109/AITest.2019.000-3
JO  - 2019 IEEE International Conference On Artificial Intelligence Testing (AITest)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE International Conference On Artificial Intelligence Testing (AITest)
Y1  - 4-9 April 2019
AB  - With modern advances in artificial intelligence (AI) and machine learning and their applications to software testing, the intersection of AI and testing is receiving close attention. The 2018 Annual Western Conference on Software Testing Analysis and Review featured a two-session panel on AI for Software Testing (AIST). The panel brought together six industry experts with experience developing AIST products, services, and research prototypes. Questions sourced from the industrial testing community were used to provoke thought, stimulate conversation, and guide panel discussions. This paper provides a review of the industry panel, which includes discussions on the visions, ideas, thoughts, strategies, directions, and lessons learned developing systems that use AI to test software, applying methods to test AI systems, and designing self-testing systems. Both the testing community survey and the expert panel yielded insightful perspectives on AIST in practice.
ER  - 

TY  - JOUR
TI  - Grammar Based Directed Testing of Machine Learning Systems
T2  - IEEE Transactions on Software Engineering
SP  - 2487
EP  - 2503
AU  - S. Udeshi
AU  - S. Chattopadhyay
PY  - 2021
DO  - 10.1109/TSE.2019.2953066
JO  - IEEE Transactions on Software Engineering
IS  - 11
SN  - 1939-3520
VO  - 47
VL  - 47
JA  - IEEE Transactions on Software Engineering
Y1  - 1 Nov. 2021
AB  - The massive progress of machine learning has seen its application over a variety of domains in the past decade. But how do we develop a systematic, scalable and modular strategy to validate machine-learning systems? We present, to the best of our knowledge, the first approach, which provides a systematic test framework for machine-learning systems that accepts grammar-based inputs. Our Ogma approach automatically discovers erroneous behaviours in classifiers and leverages these erroneous behaviours to improve the respective models. Ogma leverages inherent robustness properties present in any well trained machine-learning model to direct test generation and thus, implementing a scalable test generation methodology. To evaluate our Ogma approach, we have tested it on three real world natural language processing (NLP) classifiers. We have found thousands of erroneous behaviours in these systems. We also compare Ogma with a random test generation approach and observe that Ogma is more effective than such random test generation by up to 489 percent.
ER  - 

TY  - JOUR
TI  - A Literature Review of Using Machine Learning in Software Development Life Cycle Stages
T2  - IEEE Access
SP  - 140896
EP  - 140920
AU  - S. Shafiq
AU  - A. Mashkoor
AU  - C. Mayr-Dorn
AU  - A. Egyed
PY  - 2021
DO  - 10.1109/ACCESS.2021.3119746
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 9
VL  - 9
JA  - IEEE Access
Y1  - 2021
AB  - The software engineering community is rapidly adopting machine learning for transitioning modern-day software towards highly intelligent and self-learning systems. However, the software engineering community is still discovering new ways how machine learning can offer help for various software development life cycle stages. In this article, we present a study on the use of machine learning across various software development life cycle stages. The overall aim of this article is to investigate the relationship between software development life cycle stages, and machine learning tools, techniques, and types. We attempt a holistic investigation in part to answer the question of whether machine learning favors certain stages and/or certain techniques.
ER  - 

TY  - CONF
TI  - Impact of crossover and mutation on reproduction in evolutionary test model learning
T2  - 2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)
SP  - 39
EP  - 44
AU  - M. Sroka
AU  - D. Fisch
AU  - R. Nagy
PY  - 2016
DO  - 10.1109/SISY.2016.7601515
JO  - 2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)
IS  - 
SN  - 1949-0488
VO  - 
VL  - 
JA  - 2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)
Y1  - 29-31 Aug. 2016
AB  - Automation in the software test design process has a significant impact on the software testing process and therefore also on the overall software development in the industry. The focus of this paper is on the automation of test case design via model-based testing for automotive embedded software. A method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information is briefly described. This paper further investigates the impact of reproduction configuration on the evolutionary learning method.
ER  - 

TY  - CONF
TI  - Identification of patterns in the involvement of novice software developers in software testing processes
T2  - 2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)
SP  - 378
EP  - 382
AU  - G. Caiza
AU  - F. I. Torres
AU  - M. V. Garcia
PY  - 2021
DO  - 10.1109/ICAICA52286.2021.9498137
JO  - 2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)
Y1  - 28-30 June 2021
AB  - Background: Testing is often used to make decisions about whether or not to implement software projects. How the software industry effectively engages novice software developers in both general development and testing is less clear; however, it is especially important to determine the views of more experienced developers on this issue. Objectives: We seek to explore how software companies involve novice software developers in software development and testing; to understand how experienced developers involve novice programmers in testing; and to discover systematic patterns in experienced developers' opinions about the involvement of novice software developers. Methods: We conducted a survey of three private software companies and one public institution, taking into account experienced developers. We asked respondents to indicate how they involve novice software developers in their work activities, as well as their perspectives on software testing. Results: We identified 4 patterns that describe the involvement of novice programmers. We identified that, due to the lack of clear processes for involving novice developers in software testing, a key pattern that was identified is how the novice developer is limited when testing a product.
ER  - 

TY  - JOUR
TI  - Generating Python Mutants From Bug Fixes Using Neural Machine Translation
T2  - IEEE Access
SP  - 85678
EP  - 85693
AU  - S. Aşik
AU  - U. Yayan
PY  - 2023
DO  - 10.1109/ACCESS.2023.3302695
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - Due to the fast-paced development of technology, the software has become a crucial aspect of modern life, facilitating the operation and management of hardware devices. Nevertheless, using substandard software can result in severe complications for users, putting human lives at risk. This underscores the significance of error-free and premium-quality software. Verification and validation are essential in ensuring high-quality software development; software testing is integral to this process. Although code coverage is a prevalent method for assessing the efficacy of test suites, it has some limitations. Therefore, mutation testing is proposed as a remedy to tackle these limitations. Furthermore, mutation testing is recognized as a method for directing test case creation and evaluating the effectiveness of test suites. Our proposed method involves autonomously learning mutations from faults in real-world software applications. Firstly, our approach involves extracting bug fixes at the method-level, classifying them according to mutation types, and performing code abstraction. Subsequently, the approach utilizes a deep learning technique based on neural machine translation to develop mutation models. Our method has been trained and assessed using approximately  $\sim $ 588k bug fix commits extracted from GitHub. The results of our experimental assessment indicate that our models can forecast mutations resembling resolved bugs in 6% to 35% of instances. The models effectively revert fixed code to its original buggy version, reproducing the original bug and generating various other buggy codes with up to 94% accuracy. More than 96% of the generated mutants also demonstrate lexical and syntactic accuracy.
ER  - 

TY  - CONF
TI  - Trends in Software Engineering Processes using Deep Learning: A Systematic Literature Review
T2  - 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)
SP  - 445
EP  - 454
AU  - A. F. Del Carpio
AU  - L. B. Angarita
PY  - 2020
DO  - 10.1109/SEAA51224.2020.00077
JO  - 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)
Y1  - 26-28 Aug. 2020
AB  - In recent years, several researchers have applied machine learning techniques to several knowledge areas achieving acceptable results. Thus, a considerable number of deep learning models are focused on a wide range of software processes. This systematic review investigates the software processes supported by deep learning models, determining relevant results for the software community. This research identified that the most extensively investigated sub-processes are software testing and maintenance. In such sub-processes, deep learning models such as CNN, RNN, and LSTM are widely used to process bug reports, malware classification, libraries and commits recommendations generation. Some solutions are oriented to effort estimation, classify software requirements, identify GUI visual elements, identification of code authors, the similarity of source codes, predict and classify defects, and analyze bug reports in testing and maintenance processes.
ER  - 

TY  - CONF
TI  - An approach to enhance students' competency in software verification techniques
T2  - 2015 IEEE Frontiers in Education Conference (FIE)
SP  - 1
EP  - 9
AU  - O. Ochoa
AU  - S. Salamah
PY  - 2015
DO  - 10.1109/FIE.2015.7344050
JO  - 2015 IEEE Frontiers in Education Conference (FIE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 IEEE Frontiers in Education Conference (FIE)
Y1  - 21-24 Oct. 2015
AB  - In this paper we present an approach used to enhance students' competency in software verification. Students were asked to apply software verification techniques to a complex formal specification system. The complexity of the system stems from its sophisticated requirements. Selecting such system for this study was intentional for the following two reasons 1) the system is difficult to understand and analyze because of the domain knowledge required to generate formal specifications in temporal logic and 2) the system is large and complex which lends itself to a wide range of applicable verification techniques, and thus highlights the differences in the capabilities of each of the software verification approaches. Students were assessed using multiple criteria including; examination in applying learned techniques, students' attitude toward the technique, perceived efficiency of the techniques in discovering software defects, and the ability of the technique to locate errors in the code beyond simply indicating their presence. The results of this work show that the students applied the learned techniques successfully and their attitudes towards software verification improved.
ER  - 

TY  - CONF
TI  - A C++ implementation of the IPO algorithm
T2  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 72
EP  - 73
AU  - K. Takemura
PY  - 2022
DO  - 10.1109/ICSTW55395.2022.00026
JO  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 4-13 April 2022
AB  - The paper reports a C++ program for generating combinatorial test suites which I wrote for the purpose of learning combinatorial testing. The program is basically a direct implementation of the IPO strategy except for some optimization techniques. Unlike in the original strategy, for example, don’t cares are filled with some specific values that are heuristically chosen.
ER  - 

TY  - CONF
TI  - An Intelligent Monitoring Algorithm to Detect Dependencies between Test Cases in the Manual Integration Process
T2  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 353
EP  - 360
AU  - C. Landin
AU  - X. Zhao
AU  - M. Längkvist
AU  - A. Loutfi
PY  - 2023
DO  - 10.1109/ICSTW58534.2023.00066
JO  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 16-20 April 2023
AB  - Finding a balance between meeting test coverage and minimizing the testing resources is always a challenging task both in software (SW) and hardware (HW) testing. Therefore, employing machine learning (ML) techniques for test optimization purposes has received a great deal of attention. However, utilizing machine learning techniques frequently requires large volumes of valuable data to be trained. Although, the data gathering is hard and also expensive, manual data analysis takes most of the time in order to locate the source of failure once they have been produced in the so-called fault localization. Moreover, by applying ML techniques to historical production test data, relevant and irrelevant features can be found using strength association, such as correlation- and mutual information-based methods. In this paper, we use production data records of 100 units of a 5G radio product containing more than 7000 test results. The obtained results show that insightful information can be found after clustering the test results by their strength association, most linear and monotonic, which would otherwise be challenging to identify by traditional manual data analysis methods.
ER  - 

TY  - CONF
TI  - An Adaptive Approach for Fault Localization using R-CNN
T2  - 2022 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)
SP  - 1
EP  - 6
AU  - D. Ghosh
AU  - J. P. Singh
AU  - J. Singh
PY  - 2022
DO  - 10.1109/ASSIC55218.2022.10088417
JO  - 2022 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)
Y1  - 19-20 Nov. 2022
AB  - Software accuracy and dependability become very important issues now-a-days. It is more difficult to identify software program errors due to the growing size and complexity of programs. Traditional fault localization methods fall short of finding all the defects in a huge real-world program. The use of machine learning in that situation is effective. In this research, we present an R-CNN model-based fault localization technique. The coverage matrix is used as the model's input during training, and it uses test data to determine each program statement's suspiciousness score. Each statement is given a rank based on its score. Three benchmark programs have been looked at to evaluate the proposed work's reliability. The analysis of the results demonstrates that the suggested method can effectively identify false statements.
ER  - 

TY  - CONF
TI  - TCP-Net++: Test Case Prioritization Using End-to-End Deep Neural Networks - Deployment Analysis and Enhancements
T2  - 2023 IEEE International Conference On Artificial Intelligence Testing (AITest)
SP  - 99
EP  - 106
AU  - M. Abdelkarim
AU  - R. ElAdawi
PY  - 2023
DO  - 10.1109/AITest58265.2023.00024
JO  - 2023 IEEE International Conference On Artificial Intelligence Testing (AITest)
IS  - 
SN  - 2835-3560
VO  - 
VL  - 
JA  - 2023 IEEE International Conference On Artificial Intelligence Testing (AITest)
Y1  - 17-20 July 2023
AB  - The increasing number of test cases and frequency of continuous integration in software projects has created a bottleneck in regression testing. To save time and hardware resources, machine learning techniques can be applied without compromising quality. In this work, we present a case study for deployment analysis and results of using our previous work: TCP-Net: Test Case Prioritization using End-to-End Deep Neural Networks [1] in a real-life industrial environment, showing roadblocks, challenges, and enhancements done to improve its performance and usability, achieving 90% to 100% failure coverage by running an average of 23% to 39% of the test cases.
ER  - 

TY  - CONF
TI  - META: Multidimensional Evaluation of Testing Ability
T2  - 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)
SP  - 139
EP  - 143
AU  - T. Zhou
AU  - J. Liu
AU  - Y. Wang
AU  - Z. Chen
PY  - 2022
DO  - 10.1145/3510454.3516867
JO  - 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)
IS  - 
SN  - 2574-1926
VO  - 
VL  - 
JA  - 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)
Y1  - 22-24 May 2022
AB  - As the market’s demand for software quality continues increasing, companies increase demand for excellent testing engineers. The on-line testing platform cultivates students by offering software testing courses. Students are encouraged to submit test codes by exams on the online testing platform during the course evaluation. However, the problem of how to effectively assess the test code written by students is to be solved. This paper implements a multidimensional evaluation system named META for software testing based on an online testing platform. META is designed to address the problem of how to evaluate testing effectiveness systematically. This paper evaluates students’ testing effectiveness in seven dimensions for three software testing types: developer unit testing, web application testing, and mobile application testing, combining test codes and test behaviours. For the validity of META, 14 exams are selected from MOOCTest for an experiment in this paper, of which ten exams for developer unit testing, three exams for mobile application testing, and one exam for web application testing, involving 718 students participating in the exam and 26666 records submitted. The experimental results show that META can present significant variability in different dimensions for different students with similar scores. Video URL: https://www.youtube.com/watch?v=EiCSMtefPMU.
ER  - 

TY  - CONF
TI  - Research on Performance Test of Chinese Character APP Software Based on OL-ADE Algorithm
T2  - 2021 5th Asian Conference on Artificial Intelligence Technology (ACAIT)
SP  - 410
EP  - 413
AU  - C. Wang
AU  - C. Zhao
AU  - Q. Fu
PY  - 2021
DO  - 10.1109/ACAIT53529.2021.9731282
JO  - 2021 5th Asian Conference on Artificial Intelligence Technology (ACAIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 5th Asian Conference on Artificial Intelligence Technology (ACAIT)
Y1  - 29-31 Oct. 2021
AB  - Entering the information age, Chinese character APP assumes the task of cultural dissemination in a novel form as a carrier. And, software performance testing, a means to ensure software quality, is needed to maintain its performance. In view of this, the research first designs an opposition-based learning of adaptive evolution based on reverse learning strategy (OL-ADE), and then builds a Chinese character APP software performance test model on this basis and put it into application, and finally verify its effect through experiments. The results show that when the population size is 50 or 100, the OL-ADE algorithm has the least number of iterations under different data input ranges, and the number of iterations is within [40, 150]. The branch coverage is 100%, and the average number of iterations required by OL-ADE algorithm is the smallest. The above results show that OL-ADE algorithm has the best overall performance among the four algorithms, and can complete the software performance test of Chinese character APP.
ER  - 

TY  - CONF
TI  - Learning to Predict Software Testability
T2  - 2021 26th International Computer Conference, Computer Society of Iran (CSICC)
SP  - 1
EP  - 5
AU  - M. Z. Nasrabadi
AU  - S. Parsa
PY  - 2021
DO  - 10.1109/CSICC52343.2021.9420548
JO  - 2021 26th International Computer Conference, Computer Society of Iran (CSICC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 26th International Computer Conference, Computer Society of Iran (CSICC)
Y1  - 3-4 March 2021
AB  - Software testability is the propensity of code to reveal its existing faults, particularly during automated testing. Testing success depends on the testability of the program under test. On the other hand, testing success relies on the coverage of the test data provided by a given test data generation algorithm. However, little empirical evidence has been shown to clarify whether and how software testability affects test coverage. In this article, we propose a method to shed light on this subject. Our proposed framework uses the coverage of Software Under Test (SUT), provided by different automatically generated test suites, to build machine learning models, determining the testability of programs based on many source code metrics. The resultant models can predict the code coverage provided by a given test data generation algorithm before running the algorithm, reducing the cost of additional testing. The predicted coverage is used as a concrete proxy to quantify source code testability. Experiments show an acceptable accuracy of 81.94% in measuring and predicting software testability.
ER  - 

TY  - JOUR
TI  - An Approach for Detecting Infeasible Paths Based on a SMT Solver
T2  - IEEE Access
SP  - 69058
EP  - 69069
AU  - S. Jiang
AU  - H. Wang
AU  - Y. Zhang
AU  - M. Xue
AU  - J. Qian
AU  - M. Zhang
PY  - 2019
DO  - 10.1109/ACCESS.2019.2918558
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 7
VL  - 7
JA  - IEEE Access
Y1  - 2019
AB  - Software testing is an important means to ensure software quality. Testers need to ensure that every component of the software is tested correctly to achieve high coverage, such as path coverage, decision coverage, and branch coverage. An infeasible path is a path that cannot be traversed by any test cases. The existence of infeasible paths can waste test resources; therefore, detection of infeasible paths are necessary before path testing. This paper presents a static method for the detecting infeasible paths that is based on a satisfiability modulo theory (SMT) solver. First, the proposed method generates a sub-path set and converts the feasibility issues into inequalities. Second, a constraint solver is used to solve the inequalities and, then, the sub-paths are divided into two categories: infeasible sub-paths and undetermined sub-paths. The paths that were expanded from the latter will be tested again to determine their feasibility. Finally, the feasibility of all paths is detected. Most of the detection works are done on the sub-path set; therefore, our method provides an effective solution to the path-explosion problem. The experimental results showed that the proposed method can detect infeasible paths more accurately and effectively than most existing methods.
ER  - 

TY  - CONF
TI  - Towards the Role of Test Design in Programming Assignments
T2  - 2017 IEEE 30th Conference on Software Engineering Education and Training (CSEE&T)
SP  - 170
EP  - 179
AU  - L. P. Scatalon
AU  - J. M. Prates
AU  - D. M. de Souza
AU  - E. F. Barbosa
AU  - R. E. Garcia
PY  - 2017
DO  - 10.1109/CSEET.2017.34
JO  - 2017 IEEE 30th Conference on Software Engineering Education and Training (CSEE&T)
IS  - 
SN  - 2377-570X
VO  - 
VL  - 
JA  - 2017 IEEE 30th Conference on Software Engineering Education and Training (CSEE&T)
Y1  - 7-9 Nov. 2017
AB  - Software testing can be very helpful to students if adopted in programming assignments throughout the Computer Science curriculum. Many testing practices involve students writing their own test cases. This approach implies that students are responsible for the test design task while performing the test activity. On the other hand, some testing practices follow the opposite approach of providing ready-made test cases, so students only need to execute and evaluate test results for their solution code. In this paper, we investigated the effect of test design in student programming performance. We conducted an experiment comparing two different testing approaches during programming assignments: student-written and instructor-provided test cases. We also assessed students' perceptions of this subject by means of a survey. Results suggest that when students are responsible for test design, i.e. when they write their own test cases, they perform better in programming assignments.
ER  - 

TY  - CONF
TI  - Analysis of the Automatic Test Generation Tool: CREST
T2  - 2016 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)
SP  - 68
EP  - 72
AU  - R. Chen
AU  - Y. Luo
AU  - R. Li
AU  - X. Zhang
AU  - L. Ying
PY  - 2016
DO  - 10.1109/ICITBS.2016.86
JO  - 2016 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)
Y1  - 17-18 Dec. 2016
AB  - The CREST is an open source automatic test generation tool for C which based on Common Intermediate Language and Yices. One of its major advantages is that several heuristic search strategies are proposed to test larger programs. The success of CREST should be attributed to static instrumentation, concolic execution, and heuristic search. Thus, it's meaningful for software testing and vulnerability exploitation to analyze this software testing platform. The concolic execution is introduced briefly in the first. Then, the work flow of CREST, instrumentation and the solver is explained. In addition, the key structures used to store symbolic expression and heuristic search strategies are given. By giving examples of executing the single-file and multiple-files program, the execution performance of three common utilities from Busybox is analyzed at last. Through this project, we have learned limitations of CREST. Conclusion and future work provide a sense of direction of concolic execution in software testing.
ER  - 

TY  - CONF
TI  - Evaluation and assessment of effects on exploring mutation testing in programming courses
T2  - 2015 IEEE Frontiers in Education Conference (FIE)
SP  - 1
EP  - 9
AU  - R. A. P. Oliveira
AU  - L. B. R. Oliveira
AU  - B. B. P. Cafeo
AU  - V. H. S. Durelli
PY  - 2015
DO  - 10.1109/FIE.2015.7344051
JO  - 2015 IEEE Frontiers in Education Conference (FIE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 IEEE Frontiers in Education Conference (FIE)
Y1  - 21-24 Oct. 2015
AB  - Mutation analysis is a testing strategy that consists of using supporting tools to seed artificial faults in the original code of a software under test, generating faulty programs (“mutants”) that are supposed to produce incorrect outputs. Novice programmers suffer of a wide range of deficits due to defective training processes. We argue that the incorporation of experiences on mutation testing in programming courses adds valuable knowledge to the learning process. In this paper we evaluate the effects of using mutation testing to improve the learning process of students in programming courses. We present results of experiments and analysis involving undergraduate students. These experiments are the continuation of a previous work in which we raise empirical evidences that the adequate incorporation of mutation testing in programming courses contributes to form an effective environment that fosters learning. To do so, we provide a mutation testing tool to promote the practice of mutation testing by novice programmers. Through practical experiences and several analysis survey we measured the effects of using the mutation testing criterion to teach programming. In addition, we collected the opinion of senior students who already knew mutation testing concepts about their opinion on the usage of mutation concepts to teach novice programmers. Our findings reveal that the effective use of mutation analysis concepts contributes to the learning process, making students see the code as a product under development that is the result of a careful manual coding process which they need for measuring and predicting the effect of each command. The main contributions discussed in this paper are: (1) presenting results of an empirical analysis involving undergraduate students, thus giving us preliminary evidence on the effects of the novel practice; (2) exposing possible practices to explore mutation testing in programming classes, highlighting the limitations and strengths of such strategy; and (3) a mutation testing tool for educational purposes.
ER  - 

TY  - CONF
TI  - Output-Oriented Software Testing Data Generation Based on Artificial Immune Algorithm
T2  - 2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS)
SP  - 298
EP  - 303
AU  - W. Zhang
AU  - Y. Qi
AU  - Z. Dou
AU  - W. Liu
AU  - B. Wei
AU  - M. Zhang
PY  - 2019
DO  - 10.1109/DDCLS.2019.8909069
JO  - 2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS)
Y1  - 24-27 May 2019
AB  - To achieve software output domain coverage is a challenge to the functional testing of security-critical software. This paper proposed a test data generation method based on artificial immune algorithm using intelligence testing approach. Firstly, based on the analysis of the software output domain coverage problem, we gave the basic idea of applying artificial immune algorithm to software testing, and introduced the general steps of the algorithm. Secondly, we designed the main operators of the artificial immune algorithm, such as antibody affinity evaluation operator, antibody concentration evaluation operator, immune selection operator, clone operator, mutation operator, etc., to achieve the automatic generation of software test data. Finally, several software was used to verify the validity of the method. The verification results show that the method achieves the software output domain coverage, and its effect is better than the genetic algorithm and random testing.
ER  - 

TY  - CONF
TI  - A Review and Refinement of Surprise Adequacy
T2  - 2021 IEEE/ACM Third International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)
SP  - 17
EP  - 24
AU  - M. Weiss
AU  - R. Chakraborty
AU  - P. Tonella
PY  - 2021
DO  - 10.1109/DeepTest52559.2021.00009
JO  - 2021 IEEE/ACM Third International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE/ACM Third International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)
Y1  - 1-1 June 2021
AB  - Surprise Adequacy (SA) is one of the emerging and most promising adequacy criteria for Deep Learning (DL) testing. As an adequacy criterion, it has been used to assess the strength of DL test suites. In addition, it has also been used to find inputs to a Deep Neural Network (DNN) which were not sufficiently represented in the training data, or to select samples for DNN retraining. However, computation of the SA metric for a test suite can be prohibitively expensive, as it involves a quadratic number of distance calculations. Hence, we developed and released a performance-optimized, but functionally equivalent, implementation of SA, reducing the evaluation time by up to 97%. We also propose refined variants of the SA computation algorithm, aiming to further increase the evaluation speed. We then performed an empirical study on MNIST, focused on the out-of-distribution detection capabilities of SA, which allowed us to reproduce parts of the results presented when SA was first released. The experiments show that our refined variants are substantially faster than plain SA, while producing comparable outcomes. Our experimental results exposed also an overlooked issue of SA: it can be highly sensitive to the non-determinism associated with the DNN training procedure.
ER  - 

TY  - CONF
TI  - Win GUI Crawler: A tool prototype for desktop GUI image and metadata collection
T2  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 223
EP  - 228
AU  - M. Savic
AU  - M. Mäntylä
AU  - M. Claes
PY  - 2022
DO  - 10.1109/ICSTW55395.2022.00046
JO  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 4-13 April 2022
AB  - Despite the widespread of test automation, automatic testing of graphical user interfaces (GUI) remains a challenge. This is partly due to the difficulty of reliably identifying GUI elements over different versions of a given software system. Machine vision techniques could be a potential way of addressing this issue by automatically identifying GUI elements with the help of machine learning. However, developing a GUI testing tool relying on automatic identification of graphical elements first requires to acquire large amount of labeled data. In this paper, we present Win GUI Crawler, a tool for automatically gathering such data from Microsoft Windows GUI applications. The tool is based on Microsoft Windows Application Driver and performs actions on the GUI using a depth-first traversal of the GUI element tree. For each action performed by the crawler, screenshots are taken and metadata is extracted for each of the different screens. Bounding boxes of GUI elements are then filtered in order to identify what GUI elements are actually visible on the screen. Win GUI Crawler is then evaluated on several popular Windows applications and the current limitations are discussed.
ER  - 

TY  - JOUR
TI  - Survey on Learning-Based Formal Methods: Taxonomy, Applications and Possible Future Directions
T2  - IEEE Access
SP  - 108561
EP  - 108578
AU  - F. Wang
AU  - Z. Cao
AU  - L. Tan
AU  - H. Zong
PY  - 2020
DO  - 10.1109/ACCESS.2020.3000907
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 8
VL  - 8
JA  - IEEE Access
Y1  - 2020
AB  - Formal methods play an important role in testing and verifying software quality, especially in modern society with rapid technological updates. Learning-based techniques have been extensively applied to learn (a model or model-free) for formal verification and to learn system specifications, and resulted in numerous contributions. Due to the fact that adequate system models are often difficult to design manually and manual definition of specifications for such software systems gets infeasible, which motivate new research directions in learning models and/or specifications from observed system behaviors automatically. This paper mainly concentrates on learning-based techniques in formal methods area. An up-to-date overview of the current state-of-the-art in learning-based formal methods is provided in the paper. This paper is not a comprehensive survey of learning-based techniques in formal methods area, but rather as a survey of the taxonomy, applications and possible future directions in learning-based formal methods.
ER  - 

TY  - CONF
TI  - An Empirical Evaluation of Mutation Operators for Deep Learning Systems
T2  - 2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)
SP  - 74
EP  - 84
AU  - G. Jahangirova
AU  - P. Tonella
PY  - 2020
DO  - 10.1109/ICST46399.2020.00018
JO  - 2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)
Y1  - 24-28 Oct. 2020
AB  - Deep Learning (DL) is increasingly adopted to solve complex tasks such as image recognition or autonomous driving. Companies are considering the inclusion of DL components in production systems, but one of their main concerns is how to assess the quality of such systems. Mutation testing is a technique to inject artificial faults into a system, under the assumption that the capability to expose (kilt) such artificial faults translates into the capability to expose also real faults. Researchers have proposed approaches and tools (e.g., Deep-Mutation and MuNN) that make mutation testing applicable to deep learning systems. However, existing definitions of mutation killing, based on accuracy drop, do not take into account the stochastic nature of the training process (accuracy may drop even when re-training the un-mutated system). Moreover, the same mutation operator might be effective or might be trivial/impossible to kill, depending on its hyper-parameter configuration. We conducted an empirical evaluation of existing operators, showing that mutation killing requires a stochastic definition and identifying the subset of effective mutation operators together with the associated most effective configurations.
ER  - 

TY  - CONF
TI  - Automatically Classifying Test Results by Semi-Supervised Learning
T2  - 2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)
SP  - 116
EP  - 126
AU  - R. Almaghairbe
AU  - M. Roper
PY  - 2016
DO  - 10.1109/ISSRE.2016.22
JO  - 2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)
IS  - 
SN  - 2332-6549
VO  - 
VL  - 
JA  - 2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)
Y1  - 23-27 Oct. 2016
AB  - A key component of software testing is deciding whether a test case has passed or failed: an expensive and error-prone manual activity. We present an approach to automatically classify passing and failing executions using semi-supervised learning on dynamic execution data (test inputs/outputs and execution traces). A small proportion of the test data is labelled as passing or failing and used in conjunction with the unlabelled data to build a classifier which labels the remaining outputs (classify them as passing or failing tests). A range of learning algorithms are investigated using several faulty versions of three systems along with varying types of data (inputs/outputs alone, or in combination with execution traces) and different labelling strategies (both failing and passing tests, and passing tests alone). The results show that in many cases labelling just a small proportion of the test cases - as low as 10% - is sufficient to build a classifier that is able to correctly categorise the large majority of the remaining test cases. This has important practical potential: when checking the test results from a system a developer need only examine a small proportion of these and use this information to train a learning algorithm to automatically classify the remainder.
ER  - 

TY  - CONF
TI  - Data-Agnostic Local Neighborhood Generation
T2  - 2020 IEEE International Conference on Data Mining (ICDM)
SP  - 1040
EP  - 1045
AU  - R. Guidotti
AU  - A. Monreale
PY  - 2020
DO  - 10.1109/ICDM50108.2020.00122
JO  - 2020 IEEE International Conference on Data Mining (ICDM)
IS  - 
SN  - 2374-8486
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Data Mining (ICDM)
Y1  - 17-20 Nov. 2020
AB  - Synthetic data generation has been widely adopted in software testing, data privacy, imbalanced learning, machine learning explanation, etc. In such contexts, it is important to generate data samples located within “local” areas surrounding specific instances. Local synthetic data can help the learning phase of predictive models, and it is fundamental for methods explaining the local behavior of obscure classifiers. The contribution of this paper is twofold. First, we introduce a method based on generative operators allowing the synthetic neighborhood generation by applying specific perturbations on a given input instance. The key factor consists in performing a data transformation that makes applicable to any type of data, i.e., data-agnostic. Second, we design a framework for evaluating the goodness of local synthetic neighborhoods exploiting both supervised and unsupervised methodologies. A deep experimentation shows the effectiveness of the proposed method.
ER  - 

TY  - CONF
TI  - Fairness-aware Configuration of Machine Learning Libraries
T2  - 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
SP  - 909
EP  - 920
AU  - S. Tizpaz-Niari
AU  - A. Kumar
AU  - G. Tan
AU  - A. Trivedi
PY  - 2022
DO  - 10.1145/3510003.3510202
JO  - 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
IS  - 
SN  - 1558-1225
VO  - 
VL  - 
JA  - 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Y1  - 25-27 May 2022
AB  - This paper investigates the parameter space of machine learning (ML) algorithms in aggravating or mitigating fairness bugs. Data-driven software is increasingly applied in social-critical applications where ensuring fairness is of paramount importance. The existing approaches focus on addressing fairness bugs by either modifying the input dataset or modifying the learning algorithms. On the other hand, the selection of hyperparameters, which provide finer controls of ML algorithms, may enable a less intrusive approach to influence the fairness. Can hyperparameters amplify or suppress discrimination present in the input dataset? How can we help programmers in detecting, understanding, and exploiting the role of hyperparameters to improve the fairness? We design three search-based software testing algorithms to un-cover the precision-fairness frontier of the hyperparameter space. We complement these algorithms with statistical debugging to explain the role of these parameters in improving fairness. We implement the proposed approaches in the tool Parfait-ML (PARameter FAIrness Testing for ML Libraries) and show its effectiveness and utility over five mature ML algorithms as used in six social-critical applications. In these applications, our approach successfully iden-tified hyperparameters that significantly improve (vis-a-vis the state-of-the-art techniques) the fairness without sacrificing precision. Surprisingly, for some algorithms (e.g., random forest), our approach showed that certain configuration of hyperparameters (e.g., restricting the search space of attributes) can amplify biases across applications. Upon further investigation, we found intuitive explanations of these phenomena, and the results corroborate simi-lar observations from the literature.
ER  - 

TY  - CONF
TI  - An Artificial Intelligence Approach to EDA Software Testing: Application to Net Delay Algorithms in FPGAs
T2  - 20th International Symposium on Quality Electronic Design (ISQED)
SP  - 311
EP  - 316
AU  - M. Raman
AU  - N. Abdallah
AU  - J. Dunoyer
PY  - 2019
DO  - 10.1109/ISQED.2019.8697652
JO  - 20th International Symposium on Quality Electronic Design (ISQED)
IS  - 
SN  - 1948-3287
VO  - 
VL  - 
JA  - 20th International Symposium on Quality Electronic Design (ISQED)
Y1  - 6-7 March 2019
AB  - In this paper we demonstrate how artificial intelligence approaches such as machine learning presents an efficient technique for testing key aspects of EDA software and for validating its quality of results, especially in the context of FPGA architectures. To do so, we describe how we apply a machine learning algorithm, namely random forest, to the case of testing and quality verification of an AWE-based delay estimation algorithm when applied to FPGA routing nets. The proposed testing application uses a machine learning model to identify potential net delay calculation errors, which then are flagged for an in-depth targeted verification. The ML model is trained using SPICE, a golden delay calculation reference. The ML features are derived from the regularity and repeatability found in FPGAs. Results obtained on a 28nm FPGA testing data set are very promising, and show higher than 97% detection rate of randomly injected errors.
ER  - 

TY  - CONF
TI  - Metamorphic Testing 20 Years Later: A Hands-on Introduction
T2  - 2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)
SP  - 538
EP  - 539
AU  - S. Segura
AU  - Z. Q. Zhou
PY  - 2018
DO  - 
JO  - 2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)
IS  - 
SN  - 2574-1934
VO  - 
VL  - 
JA  - 2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)
Y1  - 27 May-3 June 2018
AB  - Two of the key challenges in software testing are the automated generation of test cases, and the identification of failures by checking test outputs. Both challenges are effectively addressed by metamorphic testing (MT), a software testing technique where failures are not revealed by checking an individual concrete output, but by checking the relations among the inputs and outputs of multiple executions of the software under test. Two decades after its introduction, MT is becoming a fully-fledged testing paradigm with successful applications in multiple domains including, among others, big data engineering, simulation and modeling, compilers, machine learning programs, autonomous cars and drones, and cybersecurity. This technical briefing will provide an introduction to MT from a double perspective. First, we will present the technique and the results of a novel survey outlining its main trends and lessons learned. Then, we will go deeper and present some of the successful applications of the technique, as well as challenges and opportunities on the topic. The briefing will be complemented with practical exercises on testing real web applications and APIs.
ER  - 

TY  - CONF
TI  - Prioritized Test Generation Guided by Software Fault Prediction
T2  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 218
EP  - 225
AU  - E. Hershkovich
AU  - R. Stern
AU  - R. Abreu
AU  - A. Elmishali
PY  - 2021
DO  - 10.1109/ICSTW52544.2021.00045
JO  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 12-16 April 2021
AB  - Writing and running software unit tests is one of the fundamental techniques used to maintain software quality. However, this process is rather costly and time consuming. Thus, much effort has been devoted to generating unit tests automatically. The common objective of test generation algorithms is to maximize code coverage. However, maximizing coverage is not necessarily correlated with identifying faults [1]. In this work, we propose a novel approach for test generation aiming at generating a small set of tests that cover the software components that are likely to contain bugs. To identify which components are more likely to contain bugs, we train a software fault prediction model using machine learning techniques. We implemented this approach in practice in a tool called QUADRANT, and demonstrate its effectiveness on five real-world, open-source projects. Results show the benefit of using QUADRANT, where test generation guided by our fault prediction model can detect more than double the number of bugs compared to a coverage-oriented approach, thereby saving test generation and execution efforts.
ER  - 

TY  - CONF
TI  - DTLS-Fuzzer: A DTLS Protocol State Fuzzer
T2  - 2022 IEEE Conference on Software Testing, Verification and Validation (ICST)
SP  - 456
EP  - 458
AU  - P. Fiterău-Broştean
AU  - B. Jonsson
AU  - K. Sagonas
AU  - F. Tåquist
PY  - 2022
DO  - 10.1109/ICST53961.2022.00051
JO  - 2022 IEEE Conference on Software Testing, Verification and Validation (ICST)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2022 IEEE Conference on Software Testing, Verification and Validation (ICST)
Y1  - 4-14 April 2022
AB  - DTLS-Fuzzer is a protocol state fuzzer for imple-mentations of DTLS clients and servers. DTLS-Fuzzer uses model learning to generate a state machine model of a DTLS implementation, capturing its input/output behavior. This model can be used for model-based testing or can be analyzed for security vulnerabilities and specification violations. This demo abstract overviews the architecture, API, and usage of the tool.
ER  - 

TY  - CONF
TI  - One-Size-Fits-None? Improving Test Generation Using Context-Optimized Fitness Functions
T2  - 2019 IEEE/ACM 12th International Workshop on Search-Based Software Testing (SBST)
SP  - 3
EP  - 4
AU  - G. Gay
PY  - 2019
DO  - 10.1109/SBST.2019.000-1
JO  - 2019 IEEE/ACM 12th International Workshop on Search-Based Software Testing (SBST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE/ACM 12th International Workshop on Search-Based Software Testing (SBST)
Y1  - 26-27 May 2019
AB  - Current approaches to search-based test case generation have yielded limited results in terms of human-competitiveness. However, effective search-based test generation relies on the selection of the correct fitness functions-feedback mechanisms-for a chosen goal. We propose that the key to overcoming these limitations lies in infusing domain knowledge and context into the fitness functions used to guide the search and the ability to automatically optimize the fitness functions used when generating tests for a given class, goal, and algorithm.
ER  - 

TY  - CONF
TI  - A Study of Oracle Approximations in Testing Deep Learning Libraries
T2  - 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
SP  - 785
EP  - 796
AU  - M. Nejadgholi
AU  - J. Yang
PY  - 2019
DO  - 10.1109/ASE.2019.00078
JO  - 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
IS  - 
SN  - 2643-1572
VO  - 
VL  - 
JA  - 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
Y1  - 11-15 Nov. 2019
AB  - Due to the increasing popularity of deep learning (DL) applications, testing DL libraries is becoming more and more important. Different from testing general software, for which output is often asserted definitely (e.g., an output is compared with an oracle for equality), testing deep learning libraries often requires to perform oracle approximations, i.e., the output is allowed to be within a restricted range of the oracle. However, oracle approximation practices have not been studied in prior empirical work that focuses on traditional testing practices. The prevalence, common practices, maintenance and evolution challenges of oracle approximations remain unknown in literature. In this work, we study oracle approximation assertions implemented to test four popular DL libraries. Our study shows that there exists a non-negligible portion of assertions that leverage oracle approximation in the test cases of DL libraries. Also, we identify the common sources of oracles on which oracle approximations are being performed through a comprehensive manual study. Moreover, we find that developers frequently modify code related to oracle approximations, i.e., using a different approximation API, modifying the oracle or the output from the code under test, and using a different approximation threshold. Last, we performed an in-depth study to understand the reasons behind the evolution of oracle approximation assertions. Our findings reveal important maintenance challenges that developers may face when maintaining oracle approximation practices as code evolves in DL libraries.
ER  - 

TY  - CONF
TI  - Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty
T2  - 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
SP  - 739
EP  - 751
AU  - X. Zhang
AU  - X. Xie
AU  - L. Ma
AU  - X. Du
AU  - Q. Hu
AU  - Y. Liu
AU  - J. Zhao
AU  - M. Sun
PY  - 2020
DO  - 10.1145/3377811.3380368
JO  - 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
IS  - 
SN  - 1558-1225
VO  - 
VL  - 
JA  - 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
Y1  - 5-11 Oct. 2020
AB  - Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by ourmethod is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.
ER  - 

TY  - CONF
TI  - Bug Hide-and-Seek: An Educational Game for Investigating Verification Accuracy in Software Tests
T2  - 2018 IEEE Frontiers in Education Conference (FIE)
SP  - 1
EP  - 8
AU  - K. Buffardi
AU  - P. Valdivia
PY  - 2018
DO  - 10.1109/FIE.2018.8658748
JO  - 2018 IEEE Frontiers in Education Conference (FIE)
IS  - 
SN  - 2377-634X
VO  - 
VL  - 
JA  - 2018 IEEE Frontiers in Education Conference (FIE)
Y1  - 3-6 Oct. 2018
AB  - This Innovative Practice Full Paper describes a pedagogical technique for introducing unit testing within software engineering courses. The Bug Hide-and-Seek educational game reinforces testing principles by requiring students to develop some correct solutions as well as some other solutions that intentionally contain bugs. While developing the correct and buggy solutions, students also write corresponding tests that should identify whether each solution contains bugs or exhibits acceptable behavior. Consequently, the first goal of the game is to hide a clever bug that will trick other students' tests into passing the implementation, despite the hidden bug. The second goal is to write thorough tests that can accurately differentiate correct from incorrect software behavior. We introduce the motivation, pedagogy, and preliminary analysis of two variations of the Bug Hide-and-Seek game, while comparing their tradeoffs. The between-subject variation considers each student's test suite in its entirety. The within-subject variation requires more sophisticated analysis, but considers each individual function along with its corresponding tests, which provides more granular insight and specific feedback to students. We conducted a pilot study of both variations over two semesters of students (n=87) playing the Bug Hide-and-Seek game. We found that students' test True Positive Rate and True Negative Rate at verifying implementations are both significant predictors of a lack of bugs in their own solution.
ER  - 

TY  - CONF
TI  - Regression Test Generation by Usage Coverage Driven Clustering on User Traces
T2  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 82
EP  - 89
AU  - F. Tamagnan
AU  - F. Bouquet
AU  - A. Vernotte
AU  - B. Legeard
PY  - 2023
DO  - 10.1109/ICSTW58534.2023.00026
JO  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 16-20 April 2023
AB  - Regression testing is used to verify that changes or modifications do not negatively impact existing features or functionality. However, it can be a significant bottleneck due to the challenge of determining the optimal number of tests for adequate coverage and fast execution and the difficulty for Quality Assurance engineers to anticipate end-user behavior. This paper presents a Usage Coverage Driven Clustering (UCDC) approach that regroups user trace logs in clusters of similar behavior, then constitutes the optimal test suite by electing one user trace per cluster as test cases. To find the optimal number of clusters, a novel metric is proposed to evaluate the statistical representativeness of the selected test candidates. Experimentation on three datasets 1 shows that the proposed metric, as compared to generic metrics such as the David-Bouldin index, leads to a smaller number of clusters while ensuring 95% usage coverage by the test suite. Several approaches from the state of the art are combined with UCDC and compared, and two of them outperform the others.
ER  - 

TY  - CONF
TI  - Efficient and Effective Generation of Test Cases for Pedestrian Detection - Search-based Software Testing of Baidu Apollo in SVL
T2  - 2021 IEEE International Conference on Artificial Intelligence Testing (AITest)
SP  - 103
EP  - 110
AU  - H. Ebadi
AU  - M. H. Moghadam
AU  - M. Borg
AU  - G. Gay
AU  - A. Fontes
AU  - K. Socha
PY  - 2021
DO  - 10.1109/AITEST52744.2021.00030
JO  - 2021 IEEE International Conference on Artificial Intelligence Testing (AITest)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Artificial Intelligence Testing (AITest)
Y1  - 23-26 Aug. 2021
AB  - With the growing capabilities of autonomous vehicles, there is a higher demand for sophisticated and pragmatic quality assurance approaches for machine learning-enabled systems in the automotive AI context. The use of simulation-based prototyping platforms provides the possibility for early-stage testing, enabling inexpensive testing and the ability to capture critical corner-case test scenarios. Simulation-based testing properly complements conventional on-road testing. However, due to the large space of test input parameters in these systems, the efficient generation of effective test scenarios leading to the unveiling of failures is a challenge. This paper presents a study on testing pedestrian detection and emergency braking system of the Baidu Apollo autonomous driving platform within the SVL simulator. We propose an evolutionary automated test generation technique that generates failure-revealing scenarios for Apollo in the SVL environment. Our approach models the input space using a generic and flexible data structure and benefits a multi-criteria safety-based heuristic for the objective function targeted for optimization. This paper presents the results of our proposed test generation technique in the 2021 IEEE Autonomous Driving AI Test Challenge. In order to demonstrate the efficiency and effectiveness of our approach, we also report the results from a baseline random generation technique. Our evaluation shows that the proposed evolutionary test case generator is more effective at generating failure-revealing test cases and provides higher diversity between the generated failures than the random baseline.
ER  - 

TY  - CONF
TI  - Test Automation Improvement Model - TAIM 2.0
T2  - 2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 334
EP  - 337
AU  - S. Eldh
PY  - 2020
DO  - 10.1109/ICSTW50294.2020.00060
JO  - 2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 24-28 Oct. 2020
AB  - Test Automation Improvement Model (TAIM), has been used to guide assessments and describe patterns for aspects of automation in testing. In this paper, we have updated TAIM for the next generation of autonomous software and complemented the model to use as a driver for software quality. We bring some lessons learned and describe the evolution of the model. TAIM is heavily influenced by analytics approaches, i.e. AI or machine learning, as we now strive for autonomous systems. We describe the new levels in TAIM, their focus, quality and cost aspects. Key areas are addressed and some experiences of using the TAIM process and suggestions for further development are proposed.
ER  - 

TY  - CONF
TI  - An empirical study of regression test suite reduction using MHBG_TCS tool
T2  - 2017 International Conference on Computing Networking and Informatics (ICCNI)
SP  - 1
EP  - 5
AU  - S. Singhal
AU  - B. Suri
AU  - S. Misra
PY  - 2017
DO  - 10.1109/ICCNI.2017.8123805
JO  - 2017 International Conference on Computing Networking and Informatics (ICCNI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 International Conference on Computing Networking and Informatics (ICCNI)
Y1  - 29-31 Oct. 2017
AB  - The following topics are dealt with: computer aided instruction; cloud computing; Internet; software engineering; feature extraction; mobile computing; genetic algorithms; business data processing; educational institutions; learning (artificial intelligence).
ER  - 

TY  - CONF
TI  - Grading 600+ Students: A Case Study on Peer and Self Grading
T2  - 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
SP  - 211
EP  - 220
AU  - M. Aniche
AU  - F. Mulder
AU  - F. Hermans
PY  - 2021
DO  - 10.1109/ICSE-SEET52601.2021.00031
JO  - 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
Y1  - 25-28 May 2021
AB  - Grading large classes has become a challenging and expensive task for many universities. The Delft University of Technology (TU Delft), located in the Netherlands, has observed a large increase in student numbers over the past few years. Given the large growth of the student population, grading all the submissions results in high costs. We made use of self and peer grading in the 2018-2019 edition of our software testing course. Students worked in teams of two, and self and peer graded three assignments in our course. We ended up with 906 self and peer graded submissions, which we compared to 248 submissions that were graded by our TAs. In this paper, we report on the differences we observed between self, peer, and TA grading. Our findings show that: (i) self grades tend to be 8-10% higher than peer grades on average, (ii) peer grades seem to be a good approximator of TA grades; in cases where self and peer grade differ significantly, the TA grade seems to lie in between, and (iii) the gender and the nationality of the student do not seem to affect self and peer grading.
ER  - 

TY  - CONF
TI  - Research on test case description language
T2  - 2021 IEEE International Conference on Consumer Electronics and Computer Engineering (ICCECE)
SP  - 27
EP  - 31
AU  - X. Yu
AU  - H. Wang
AU  - F. Yang
PY  - 2021
DO  - 10.1109/ICCECE51280.2021.9342169
JO  - 2021 IEEE International Conference on Consumer Electronics and Computer Engineering (ICCECE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Consumer Electronics and Computer Engineering (ICCECE)
Y1  - 15-17 Jan. 2021
AB  - Software testing is crucial in the development of software interfaces or web pages. In this paper, a test case description language (TCDL) is proposed. TCDL can conveniently describe the process of web UI testing with a grammar that is close to natural language and conforms to manual operation logic. In this paper, the manual UI testing process is abstracted by TCDL, and the syntax specification of TCDL is designed, and the parsing of TCDL is realized with the help of ANTLR tool. Using TCDL, testers can quickly write test scripts with manual test logic. TCDL reduces the learning cost of users and improves the testing efficiency.
ER  - 

TY  - CONF
TI  - Analyzing and interpreting the fault localized using PCA with CK metrics
T2  - 2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC)
SP  - 575
EP  - 580
AU  - M. S. Bajwa
AU  - P. K. Singh
AU  - A. P. Agarwal
PY  - 2016
DO  - 10.1109/PDGC.2016.7913189
JO  - 2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC)
Y1  - 22-24 Dec. 2016
AB  - The aim of this research paper is to implement soft computing method to help in software testing and decide their ease of usage and effectiveness. Quality estimation incorporates assessing dependability and also viability of programming. Dependability is regularly measured as the quantity of faults. In this paper an evolution of value measurements suites to be specific CK, and after that selecting a few metrics and disposes of different measurements in view of the definition and capacity of the measurements. Our technique accepts the presence of a faulty runs. It then chooses as indicated by a paradigm the right run that most looks like the incorrect run, analyzes the results relating to these two runs. Our technique is broadly divided on the grounds that it doesn't require any learning of the project data and no more data from the user than a classification of the records separating the it as either “correct” or “faulty”. To tentatively accept the reasonability of the strategy and, utilizing essential classes and their values. Numerous measurements have been proposed identified with different develops like class, cohesion coupling, inheritance, information hiding and polymorphism. This addresses these necessities through the improvement and usage of a suite of measurements for OO design. Object-oriented measurements require the utilization of classes. With significant usage of PCA gives a guide to how to lessen an unpredictable information set to a lower measurement to uncover, resulting simplified structures that regularly underlie measurements suite for metrics of Chidamber and Kemerer is somewhat assessed by applying standards of measurement theory. This Paper shows the utilization of PCA in programming quality estimation utilizing object-oriented measurements.
ER  - 

TY  - CONF
TI  - Design and Implementation of Software Test Laboratory Based on Cloud Platform
T2  - 2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)
SP  - 138
EP  - 144
AU  - W. Wen
AU  - J. Sun
AU  - Y. Li
AU  - P. Gu
AU  - J. Xu
PY  - 2019
DO  - 10.1109/QRS-C.2019.00039
JO  - 2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)
Y1  - 22-26 July 2019
AB  - Software testing has become an essential indicator of students' professional level. To improve students' software testing ability, the construction of software testing laboratory has become an urgent need for many institutions. With the development and popularization of cloud computing, more and more software industries rely on cloud computing, which also brings new ideas for the construction and development of software testing laboratories in colleges and universities. This paper designs and implements a software testing laboratory based on a cloud platform, which can reuse the traditional hardware devices such as the servers in the laboratory. It is an intelligent and convenient laboratory environment with rich functions and powerful management functions, such as antagonistic experiments, programming communities, developer testing, and so on. Students can also connect to the cloud platform through the campus network to carry out various experiments. The experimental results of students will be stored on the cloud platform to facilitate the management of teachers, which can effectively improve the level of resource utilization of university laboratories. Through the cloud platform, we can arouse the enthusiasm of students and improve their professional skills, and improve the quality of experimental teaching in colleges and universities as well.
ER  - 

TY  - JOUR
TI  - Semiautomated Metamorphic Testing Approach for Geographic Information Systems: An Empirical Study
T2  - IEEE Transactions on Reliability
SP  - 657
EP  - 673
AU  - Z. -W. Hui
AU  - S. Huang
AU  - C. Chua
AU  - T. Y. Chen
PY  - 2020
DO  - 10.1109/TR.2019.2931561
JO  - IEEE Transactions on Reliability
IS  - 2
SN  - 1558-1721
VO  - 69
VL  - 69
JA  - IEEE Transactions on Reliability
Y1  - June 2020
AB  - A geographic information system (GIS) provides basic location-enabled services for many different applications related to navigation, education, and telecommunications. It is a foundation for analysis and visualization. Testing GIS is critical, but challenging due to the difficulty to assess the correctness of GIS outputs, which is called the test oracle problem of software testing. Metamorphic testing alleviates the problem by constructing metamorphic relations (MRs) among multiple inputs and outputs of the program under test. In this article, a semiautomated metamorphic testing (SAMT) method, based on the formal MR model and an improved adaptive random testing algorithm, was proposed to the GIS. To evaluate the performance of our approach, we conducted a case study on a superficial area calculation program, a typical component of GIS. Six kinds of MR construction methods were suggested for the GIS domain program testing. The experimental results show that SAMT can detect the mutations effectively that could solve the test oracle problem efficiently. More importantly, there is no need to manual participation in the testing process, except for the MR construction.
ER  - 

TY  - CONF
TI  - DeepMutation: Mutation Testing of Deep Learning Systems
T2  - 2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE)
SP  - 100
EP  - 111
AU  - L. Ma
AU  - F. Zhang
AU  - J. Sun
AU  - M. Xue
AU  - B. Li
AU  - F. Juefei-Xu
AU  - C. Xie
AU  - L. Li
AU  - Y. Liu
AU  - J. Zhao
AU  - Y. Wang
PY  - 2018
DO  - 10.1109/ISSRE.2018.00021
JO  - 2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE)
IS  - 
SN  - 2332-6549
VO  - 
VL  - 
JA  - 2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE)
Y1  - 15-18 Oct. 2018
AB  - Deep learning (DL) defines a new data-driven programming paradigm where the internal system logic is largely shaped by the training data. The standard way of evaluating DL models is to examine their performance on a test dataset. The quality of the test dataset is of great importance to gain confidence of the trained models. Using an inadequate test dataset, DL models that have achieved high test accuracy may still lack generality and robustness. In traditional software testing, mutation testing is a well-established technique for quality evaluation of test suites, which analyzes to what extent a test suite detects the injected faults. However, due to the fundamental difference between traditional software and deep learning-based software, traditional mutation testing techniques cannot be directly applied to DL systems. In this paper, we propose a mutation testing framework specialized for DL systems to measure the quality of test data. To do this, by sharing the same spirit of mutation testing in traditional software, we first define a set of source-level mutation operators to inject faults to the source of DL (i.e., training data and training programs). Then we design a set of model-level mutation operators that directly inject faults into DL models without a training process. Eventually, the quality of test data could be evaluated from the analysis on to what extent the injected faults could be detected. The usefulness of the proposed mutation testing techniques is demonstrated on two public datasets, namely MNIST and CIFAR-10, with three DL models.
ER  - 

TY  - JOUR
TI  - Defect Prediction With Semantics and Context Features of Codes Based on Graph Representation Learning
T2  - IEEE Transactions on Reliability
SP  - 613
EP  - 625
AU  - J. Xu
AU  - F. Wang
AU  - J. Ai
PY  - 2021
DO  - 10.1109/TR.2020.3040191
JO  - IEEE Transactions on Reliability
IS  - 2
SN  - 1558-1721
VO  - 70
VL  - 70
JA  - IEEE Transactions on Reliability
Y1  - June 2021
AB  - To optimize the process of software testing and to improve software quality and reliability, many attempts have been made to develop more effective methods for predicting software defects. Previous work on defect prediction has used machine learning and artificial software metrics. Unfortunately, artificial metrics are unable to represent the features of syntactic, semantic, and context information of defective modules. In this article, therefore, we propose a practical approach for identifying software defect patterns via the combination of semantics and context information using abstract syntax tree representation learning. Graph neural networks are also leveraged to capture the latent defect information of defective subtrees, which are pruned based on a fix-inducing change. To validate the proposed approach for predicting defects, we define mining rules based on the GitHub workflow and collect 6052 defects from 307 projects. The experiments indicate that the proposed approach performs better than the state-of-the-art approach and five traditional machine learning baselines. An ablation study shows that the information about code concepts leads to a significant increase in accuracy.
ER  - 

TY  - JOUR
TI  - A Probabilistic Analysis of the Efficiency of Automated Software Testing
T2  - IEEE Transactions on Software Engineering
SP  - 345
EP  - 360
AU  - M. Böhme
AU  - S. Paul
PY  - 2016
DO  - 10.1109/TSE.2015.2487274
JO  - IEEE Transactions on Software Engineering
IS  - 4
SN  - 1939-3520
VO  - 42
VL  - 42
JA  - IEEE Transactions on Software Engineering
Y1  - 1 April 2016
AB  - We study the relative efficiencies of the random and systematic approaches to automated software testing. Using a simple but realistic set of assumptions, we propose a general model for software testing and define sampling strategies for random (R) and systematic (S0) testing, where each sampling is associated with a sampling cost: 1 and c units of time, respectively. The two most important goals of software testing are: (i) achieving in minimal time a given degree of confidence x in a program's correctness and (ii) discovering a maximal number of errors within a given time bound n̂. For both (i) and (ii), we show that there exists a bound on c beyond which R performs better than S0 on the average. Moreover for (i), this bound depends asymptotically only on x. We also show that the efficiency of R can be fitted to the exponential curve. Using these results we design a hybrid strategy H that starts with R and switches to S0 when S0 is expected to discover more errors per unit time. In our experiments we find that H performs similarly or better than the most efficient of both and that S0 may need to be significantly faster than our bounds suggest to retain efficiency over R.
ER  - 

TY  - CONF
TI  - Early Identification of Vulnerable Software Components via Ensemble Learning
T2  - 2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)
SP  - 476
EP  - 481
AU  - Y. Pang
AU  - X. Xue
AU  - A. S. Namin
PY  - 2016
DO  - 10.1109/ICMLA.2016.0084
JO  - 2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)
Y1  - 18-20 Dec. 2016
AB  - Software components, which are vulnerable to being exploited, need to be identified and patched. Employing any prevention techniques designed for the purpose of detecting vulnerable software components in early stages can reduce the expenses associated with the software testing process significantly and thus help building a more reliable and robust software system. Although previous studies have demonstrated the effectiveness of adapting prediction techniques in vulnerability detection, the feasibility of those techniques is limited mainly because of insufficient training data sets. This paper proposes a prediction technique targeting at early identification of potentially vulnerable software components. In the proposed scheme, the potentially vulnerable components are viewed as mislabeled data that may contain true but not yet observed vulnerabilities. The proposed hybrid technique combines the supports vector machine algorithm and ensemble learning strategy to better identify potential vulnerable components. The proposed vulnerability detection scheme is evaluated using some Java Android applications. The results demonstrated that the proposed hybrid technique could identify potentially vulnerable classes with high precision and relatively acceptable accuracy and recall.
ER  - 

TY  - CONF
TI  - Marco Polo - A Tool for Automated Exploratory Testing of Previously Unseen Online Stores
T2  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 377
EP  - 380
AU  - C. Gatt
AU  - M. Micallef
AU  - M. Bugeja
PY  - 2023
DO  - 10.1109/ICSTW58534.2023.00070
JO  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 16-20 April 2023
AB  - Online stores continue to increase in popularity with 2.3 billion people estimated to have shopped online during 2022. Whilst every online store is unique in its own right, the online shopping domain itself is quite constrained in terms of the type of functionality being offered. This begs the question as to why companies invest so much time, effort and money into developing complete test suites for their specific systems. In this paper, we argue that it would be more efficient for companies to leverage the abundant common ground between most systems in the domain such that they need only focus on features that make their product unique when it comes to allocating testing effort. We go on to present Marco Polo, an automated exploratory testing tool inspired by the renowned explorer and trader from the 13th century. Using behavioural cloning to train on expert traces from a sample of online stores, this tool is able to exploratory-test previously unseen websites whilst handling reactive websites and events such as cookie/GDPR consents. We discuss design decisions as well as challenges and opportunities for further development. A demo URL is also provided.1.
ER  - 

TY  - JOUR
TI  - An Integration Test Order Strategy to Consider Control Coupling
T2  - IEEE Transactions on Software Engineering
SP  - 1350
EP  - 1367
AU  - S. Jiang
AU  - M. Zhang
AU  - Y. Zhang
AU  - R. Wang
AU  - Q. Yu
AU  - J. W. Keung
PY  - 2021
DO  - 10.1109/TSE.2019.2921965
JO  - IEEE Transactions on Software Engineering
IS  - 7
SN  - 1939-3520
VO  - 47
VL  - 47
JA  - IEEE Transactions on Software Engineering
Y1  - 1 July 2021
AB  - Integration testing is a very important step in software testing. Existing methods evaluate the stubbing cost for class integration test orders by considering only the interclass direct relationships such as inheritance, aggregation, and association, but they omit the interclass indirect relationship caused by control coupling, which can also affect the test orders and the stubbing cost. In this paper, we introduce an integration test order strategy to consider control coupling. We advance the concept of transitive relationship to describe this kind of interclass dependency and propose a new measurement method to estimate the complexity of control coupling, which is the complexity of stubs created for a transitive relationship. We evaluate our integration test order strategy on 10 programs on various scales. The results show that considering the transitive relationship when generating class integration test orders can significantly reduce the stubbing cost for most programs and that our integration test order strategy obtains satisfactory results more quickly than other methods.
ER  - 

TY  - CONF
TI  - METHODS2TEST: A dataset of focal methods mapped to test cases
T2  - 2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)
SP  - 299
EP  - 303
AU  - M. Tufano
AU  - S. K. Deng
AU  - N. Sundaresan
AU  - A. Svyatkovskiy
PY  - 2022
DO  - 10.1145/3524842.3528009
JO  - 2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)
IS  - 
SN  - 2574-3864
VO  - 
VL  - 
JA  - 2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)
Y1  - 23-24 May 2022
AB  - Unit testing is an essential part of the software development process, which helps to identify issues with source code in early stages of development and prevent regressions. Machine learning has emerged as viable approach to help software developers generate automated unit tests. However, generating reliable unit test cases that are semantically correct and capable of catching software bugs or unintended behavior via machine learning requires large, metadata-rich, datasets. In this paper we present Methods2Test: a large, supervised dataset of test cases mapped to corresponding methods under test (i.e., focal methods). This dataset contains 780,944 pairs of JUnit tests and focal methods, extracted from a total of 91,385 Java open source projects hosted on GitHub with licenses permitting re-distribution. The main challenge behind the creation of the Methods2Test was to establish a reliable mapping between a test case and the relevant focal method. To this aim, we designed a set of heuristics, based on developers' best practices in software testing, which identify the likely focal method for a given test case. To facilitate further analysis, we store a rich set of metadata for each method-test pair in JSON-formatted files. Additionally, we extract textual corpus from the dataset at different context levels, which we provide both in raw and tokenized forms, in order to enable researchers to train and evaluate machine learning models for Automated Test Generation. Methods2Test is publicly available at: https://github.com/microsoft/methods2test
ER  - 

TY  - CONF
TI  - Regression Testing Approaches, Tools, and Applications in Various Environments
T2  - 2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)
SP  - 1
EP  - 6
AU  - V. Tomar
AU  - M. Bansal
AU  - P. Singh
PY  - 2022
DO  - 10.1109/AIST55798.2022.10064753
JO  - 2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)
Y1  - 9-10 Dec. 2022
AB  - In the entire process of developing software, the very crucial aspect is to perform a rigorous testing of the software. Regression testing is a kind of software testing that makes sure an application continues to work as intended even after upgrades, modifications, or improvements to the code. Regression testing evaluates revised software to ensure that the program's altered sections do not cause unforeseen obstacles. When there is a continual transition in the program, this test is critical. This paper discusses regression testing significance, approaches, and the latest tools. As we all know, the most crucial phase of the life cycle of a software development process is the maintenance phase. The core team of developers is responsible for maintaining the product that they provide to their clients during this phase only. Regression tests are needed after the software has been revised. This paper includes a lot of accessible regression testing approaches as well as their categories. The concept of selecting test cases, minimizing test cases, and then prioritizing the same by executing regression testing on them is included in regression testing methodologies and classifications. The emergence of new technology has also technically allowed the education sector to expand. The paper also discusses the significance of regression testing and its use in different environments.
ER  - 

TY  - CONF
TI  - Automated identification of metamorphic test scenarios for an ocean-modeling application
T2  - 2020 IEEE International Conference On Artificial Intelligence Testing (AITest)
SP  - 62
EP  - 63
AU  - D. J. Hiremath
AU  - M. Claus
AU  - W. Hasselbring
AU  - W. Rath
PY  - 2020
DO  - 10.1109/AITEST49225.2020.00016
JO  - 2020 IEEE International Conference On Artificial Intelligence Testing (AITest)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference On Artificial Intelligence Testing (AITest)
Y1  - 3-6 Aug. 2020
AB  - Metamorphic testing seeks to validate software in the absence of test oracles. Our application domain is ocean modeling, where test oracles often do not exist, but where symmetries of the simulated physical systems are known. In this short paper we present work in progress for automated generation of metamorphic test scenarios using machine learning. Metamorphic testing may be expressed as f(g(X))=h(f(X)) with f being the application under test, with input data X, and with the metamorphic relation (g, h). Automatically generated metamorphic relations can be used for constructing regression tests, and for comparing different versions of the same software application. Here, we restrict to h being the identity map. Then, the task of constructing tests means finding different g which we tackle using machine learning algorithms. These algorithms typically minimize a cost function. As one possible g is already known to be the identity map, for finding a second possible g, we construct the cost function to minimize for g being a metamorphic relation and to penalize for g being the identity map. After identifying the first metamorphic relation, the procedure is repeated with a cost function rewarding g that are orthogonal to previously found metamorphic relations. For experimental evaluation, two implementations of an oceanmodeling application will be subjected to the proposed method with the objective of presenting the use of metamorphic relations to test the implementations of the applications.
ER  - 

TY  - CONF
TI  - MAF: Method-Anchored Test Fragmentation for Test Code Plagiarism Detection
T2  - 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
SP  - 110
EP  - 120
AU  - W. Sun
AU  - X. Wang
AU  - H. Wu
AU  - D. Duan
AU  - Z. Sun
AU  - Z. Chen
PY  - 2019
DO  - 10.1109/ICSE-SEET.2019.00020
JO  - 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
Y1  - 25-31 May 2019
AB  - Software engineering education becomes popular due to the rapid development of the software industry. In order to reduce learning costs and improve learning efficiency, some online practice platforms have emerged. This paper proposes a novel test code plagiarism detection technology, namely MAF, by introducing bidirectional static slicing to anchor methods under test and extract fragments of test codes. Combined with similarity measures, MAF can achieve effective plagiarism detection by avoiding massive unrelated noisy test codes. The experiment is conducted on the dataset of Mooctest, which so far has supported hundreds of test activities around the world in the past 3 years. The experimental results show that MAF can effectively improve the performance (precision, recall and F1-measure) of similarity measures for test code plagiarism detection. We believe that MAF can further expand and promote software testing education, and it can also be extended to use in test recommendation, test reuse and other engineering applications.
ER  - 

TY  - CONF
TI  - The Inversive Relationship Between Bugs and Patches: An Empirical Study
T2  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 314
EP  - 323
AU  - J. Kim
AU  - J. Park
AU  - S. Yoo
PY  - 2023
DO  - 10.1109/ICSTW58534.2023.00062
JO  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 16-20 April 2023
AB  - Software bugs1 pose an ever-present concern for developers, and patching such bugs requires a considerable amount of costs through complex operations. In contrast, introducing bugs can be an effortless job, in that even a simple mutation can easily break the Program Under Test (PUT). Existing research has considered these two opposed activities largely separately, either trying to automatically generate realistic patches to help developers, or to find realistic bugs to simulate and prevent future defects. Despite the fundamental differences between them, however, we hypothesise that they do not syntactically differ from each other when considered simply as code changes. To examine this assumption systematically, we investigate the relationship between patches and buggy commits, both generated manually and automatically, using a clustering and pattern analysis. A large scale empirical evaluation reveals that up to 70% of patches and faults can be clustered together based on the similarity between their lexical patterns; further, 44% of the code changes can be abstracted into the identical change patterns. Moreover, we investigate whether code mutation tools can be used as Automated Program Repair (APR) tools, and APR tools as code mutation tools. In both cases, the inverted use of mutation and APR tools can perform surprisingly well, or even better, when compared to their original, intended uses. For example, 89% of patches found by SequenceR, a deep learning based APR tool, can also be found by its inversion, i.e., a model trained with faults and not patches. Similarly, real fault coupling study of mutants reveals that TBar, a template based APR tool, can generate 14% and 3% more fault couplings than traditional mutation tools, PIT and Major respectively, when used as a mutation tool. Our findings suggest that the valid scope of mining code changes for either mutation or APR can be wider than previously thought.
ER  - 

TY  - CONF
TI  - Software Defect Prediction Using a Combination of Oversampling and Undersampling Methods
T2  - 2022 6th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)
SP  - 127
EP  - 132
AU  - A. F. Iswafaza
AU  - S. Rochimah
PY  - 2022
DO  - 10.1109/ICITISEE57756.2022.10057798
JO  - 2022 6th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 6th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)
Y1  - 13-14 Dec. 2022
AB  - Software quality can be improved by doing software testing, but the more features are developed the more resources are required, therefore software defect prediction (SDP) is introduced. Various kinds of machine learning methods are used to develop SDP. However, various kinds of problems arise in SDP activities, namely data redundancy, class imbalance and feature redundancy. In this study, a combination of oversampling and under-sampling (COU) model will be proposed to solve the problem of data redundancy and class imbalance. The oversampling method used is RSMOTE and the under-sampling method used is ENN. The application of the combination model will later provide a new set of datasets that are more balanced and cleaner from ambiguous, noisy and duplication of data. From the new data generated by the model, deep learning will then be applied as a prediction model. And the evaluation will be done by applying the f-measure measurement. The results of this study indicate that the COU model used gives good results in improving the quality of SDP. When compared with the average value generated by the RSMOTE model in making predictions, the COU model provides an increase in f-measure evaluation results by 11% where the average value obtained is 0.876. So, this combination method is able to improve the performance of software defect prediction activities
ER  - 

TY  - CONF
TI  - Boosting Search Based Software Testing by Using Ensemble Methods
T2  - 2018 IEEE Congress on Evolutionary Computation (CEC)
SP  - 1
EP  - 10
AU  - X. Xu
AU  - L. Jiao
AU  - Z. Zhu
PY  - 2018
DO  - 10.1109/CEC.2018.8477734
JO  - 2018 IEEE Congress on Evolutionary Computation (CEC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE Congress on Evolutionary Computation (CEC)
Y1  - 8-13 July 2018
AB  - Search Based Software Testing (SBST) formulates testing as an optimization problem, hence some search algorithms (e.g., Genetic Algorithms) can be used to tackle it. There are different types of coverage criteria, and the goal of SBST is to improve various test adequacy criteria. However, the major limitation of SBST is the insufficiently informed fitness functions and the inefficient search algorithms. Besides, although there are various fitness functions and search algorithms for SBST, there is little guidance on when to use one fitness function (resp., search algorithm) over another. To address these problems, we propose an ensemble strategy to boost the performance of SBST. In this paper, we deal with path coverage. Concretely, by combining multiple weak fitness functions, the heuristic information of the problem instances can be expressed more sufficiently, and therefore, a stronger fitness function can be obtained. On the other hand, by combining multiple complementary search algorithms, a hyper-heuristic search algorithm is generated and the search performance can be improved. The empirical study reveals the promising results of our proposal. Especially, for the paths that are very difficult to be covered, our ensemble method proposed in this paper outperforms other approaches significantly.
ER  - 

TY  - JOUR
TI  - Automatically Tagging the “AAA” Pattern in Unit Test Cases Using Machine Learning Models
T2  - IEEE Transactions on Software Engineering
SP  - 3305
EP  - 3324
AU  - C. Wei
AU  - L. Xiao
AU  - T. Yu
AU  - X. Chen
AU  - X. Wang
AU  - S. Wong
AU  - A. Clune
PY  - 2023
DO  - 10.1109/TSE.2023.3252442
JO  - IEEE Transactions on Software Engineering
IS  - 5
SN  - 1939-3520
VO  - 49
VL  - 49
JA  - IEEE Transactions on Software Engineering
Y1  - 1 May 2023
AB  - The AAA pattern (i.e., Arrange-Act-Assert) is a common and natural layout to create a test case. Following this pattern in test cases may benefit comprehension, debugging, and maintenance. The AAA structure of real-life test cases, however, may not be clear due to their high complexity. Manually labeling AAA statements in test cases is tedious. Thus, we envision that an automated approach for labeling AAA statements in existing test cases could benefit new developers and projects that practice collective code ownership and test-driven development. This paper contributes an automatic approach based on machine learning models. The “secret sauce” of this approach is a set of three learning features that are based on the semantic, syntax, and context information in test cases, derived from the manual tagging process. Thus, our approach mimics how developers may manually tag the AAA pattern of a test case. We assess the precision, recall, and F-1 score of our approach based on 449 test cases, containing about 16,612 statements, across 4 Apache open source projects. To achieve the best performance in our approach, we explore the usage of six machine learning models; the contribution of the SMOTE data balancing technique; the comparison of the three learning features; and the comparison of five different methods for calculating the semantic feature. The results show our approach is able to identify Arrangement, Action, and Assertion statements with a precision upwards of 92%, and recall up to 74%. We also summarize some experience based on our experiments—regarding the choice of machine learning models, data balancing algorithm, and feature engineering methods—which could potentially provide some reference to related future research.
ER  - 

TY  - CONF
TI  - Do the Test Smells Assertion Roulette and Eager Test Impact Students’ Troubleshooting and Debugging Capabilities?
T2  - 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
SP  - 29
EP  - 39
AU  - W. Aljedaani
AU  - M. W. Mkaouer
AU  - A. Peruma
AU  - S. Ludi
PY  - 2023
DO  - 10.1109/ICSE-SEET58685.2023.00009
JO  - 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
IS  - 
SN  - 2832-7578
VO  - 
VL  - 
JA  - 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
Y1  - 14-20 May 2023
AB  - To ensure the quality of a software system, developers perform an activity known as unit testing, where they write code (known as test cases) that verifies the individual software units that make up the system. Like production code, test cases are subject to bad programming practices, known as test smells, that hurt maintenance activities. An essential part of most maintenance activities is program comprehension which involves developers reading the code to understand its behavior to fix issues or update features. In this study, we conduct a controlled experiment with 96 undergraduate computer science students to investigate the impact of two common types of test smells, namely Assertion Roulette and Eager Test, on a student’s ability to debug and troubleshoot test case failures. Our findings show that students take longer to correct errors in production code when smells are present in their associated test cases, especially Assertion Roulette. We envision our findings supporting academia in better equipping students with the knowledge and resources in writing and maintaining high-quality test cases. Our experimental materials are available online11https://wajdialjedaani.github.io/testsmellstd/
ER  - 

TY  - CONF
TI  - ElectroLab: Electronic Laboratory System
T2  - 2015 Fifth International Conference on e-Learning (econf)
SP  - 35
EP  - 41
AU  - S. A. El Rahman
PY  - 2015
DO  - 10.1109/ECONF.2015.23
JO  - 2015 Fifth International Conference on e-Learning (econf)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 Fifth International Conference on e-Learning (econf)
Y1  - 18-20 Oct. 2015
AB  - Nowdays, technology is often looked for more efficient solutions to several challenges in our life. In universities 'managing the labs' information manually consumes a lot of the lab-user's time and effort. It is also hard to search for a lab in a timely manner or for one that has specific hardware (HW) and software (SW). Moreover, maintaining every lab and device is done randomly upon one's request, which is not regulated nor appointed. Users are frequently increasing and the procedure is getting more complicated and difficult to coordinate between them. So, This work aims to make an Electronic Laboratory (ElectroLab) System for laboratories in the College of Computer and Information Sciences (CCIS) at Princess Nourah bint Abdulrahman University (PNU), Riyadh, Saudi Arabia. Whereas the system provide different information required about each laboratory in interactive way. Using ElectroLab system all the paper work will be replaced and offer a set of key features that manages computer-related information which will prevent any deficiencies and waste of time. These key features include laboratories scheduling, reporting technical problems, control and overseeing labs equipment. Whereas, It provides different information required about each laboratory such as hardware, software, schedules, instructors. Presenting such information will help both the Information Technology (IT) Staff and faculty members in keeping track of computer laboratories and will help the management process.
ER  - 

TY  - CONF
TI  - A Tool for Mutation Analysis in Racket
T2  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 308
EP  - 313
AU  - B. Zhuang
AU  - J. Perretta
AU  - A. Guha
AU  - J. Bell
PY  - 2023
DO  - 10.1109/ICSTW58534.2023.00061
JO  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 16-20 April 2023
AB  - Racket is a functional programming language that is used to teach CS1 at many high schools and colleges. Recent research results have shown that mutation analysis can be an effective substitute for manual grading of student test cases. In order to evaluate its efficacy in our college’s introductory programming courses, we created a prototype mutation analysis tool for Racket. We describe the design and features of the tool and perform a feasibility study using two assignments from an intro CS course where student test suite thoroughness was evaluated by hand by human graders. In our results, we find a moderate correlation between mutation score and hand-grading test suite quality score and conduct a qualitative analysis to identify situations where mutation score and hand-grading score do not correlate. We find that, compared to hand-grading, mutation analysis may require more stringent adherence to the interface specified in an assignment as well as more precisely specified assignments. On the other hand, inter-reviewer reliability is a known challenge of hand-grading, and we observe several instances where hand-graders may have assigned the wrong score. Given the relatively cheap cost to providing mutation analysis feedback to students (compared to hand-grading feedback), mutation analysis still provides the opportunity to provide faster, more frequent, feedback to learners, enabling them to improve their testing practices further. Future work will study the effectiveness of various mutation operators in Racket and perform larger-scale evaluations.
ER  - 

TY  - CONF
TI  - A Classification Study on Testing and Verification of AI-based Systems
T2  - 2023 IEEE International Conference On Artificial Intelligence Testing (AITest)
SP  - 1
EP  - 8
AU  - E. De Angelis
AU  - G. De Angelis
AU  - M. Proietti
PY  - 2023
DO  - 10.1109/AITest58265.2023.00010
JO  - 2023 IEEE International Conference On Artificial Intelligence Testing (AITest)
IS  - 
SN  - 2835-3560
VO  - 
VL  - 
JA  - 2023 IEEE International Conference On Artificial Intelligence Testing (AITest)
Y1  - 17-20 July 2023
AB  - Recent advances in Artificial Intelligence (AI) have paved the way for the development of new generations of self-adaptive systems that embed learning behaviours. Often these systems make use of Machine Learning (ML) models and algorithms, others make use of symbolic reasoning, or a combination of the two. A problem common to all these solutions is the difficulty in establishing clear conformance criteria that can be used to reliably assess whether an AI-based software system (and, in particular, ML-based) is behaving as intended, i.e., according to its specification. Research communities from different areas are investigating innovative V&V approaches in order to assess evolving AI systems against their expected functionalities. This empirical study identifies, collects and categorises relevant research papers on testing and formal verification of AI-based software systems. In total, we have considered a set of 78 fully qualified primary studies from the digital library Scopus. For each of them, we have mapped their key aspects into a classification framework that supports their comparison across a set of common dimensions.
ER  - 

TY  - CONF
TI  - Framework for Multiple User Acceptance Testing to Avoid Chaos
T2  - 2019 IEEE International Conference on System, Computation, Automation and Networking (ICSCAN)
SP  - 1
EP  - 6
AU  - N. S. R. Pillai
AU  - R. R. Hemamalini
AU  - V. Padmavathy
AU  - N. S.
PY  - 2019
DO  - 10.1109/ICSCAN.2019.8878803
JO  - 2019 IEEE International Conference on System, Computation, Automation and Networking (ICSCAN)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on System, Computation, Automation and Networking (ICSCAN)
Y1  - 29-30 March 2019
AB  - Test quality is an essential for accomplishing creation framework superiority. The idea of value is in various dimensions, the vast majority of the exertion in trying setting has been tested near estimating test adequacy. In the testing of the software for every project, user acceptance testing (UAT) is a era of program design progression in which the invention is tried in ”this present reality” by the goal cluster. The meetings of the primary clients are referred back to the plans who roll out positive developments earlier clearing the product industrially. In SDLC life cycle this UAT should be done statistically. Normally there are 3 main testing levels in UAT which are Unit testing, Integration testing and System testing. While we test the software at every level of testing the overall cost is reduced for the testing in addition to the agile model practices.
ER  - 

TY  - CONF
TI  - Learning How to Search: Generating Exception-Triggering Tests Through Adaptive Fitness Function Selection
T2  - 2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)
SP  - 63
EP  - 73
AU  - H. Almulla
AU  - G. Gay
PY  - 2020
DO  - 10.1109/ICST46399.2020.00017
JO  - 2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)
Y1  - 24-28 Oct. 2020
AB  - Search-based test generation is guided by feedback from one or more fitness functions-scoring functions that judge solution optimality. Choosing informative fitness functions is crucial to meeting the goals of a tester. Unfortunately, many goals-such as forcing the class-under-test to throw exceptions- do not have a known fitness function formulation. We propose that meeting such goals requires treating fitness function identification as a secondary optimization step. An adaptive algorithm that can vary the selection of fitness functions could adjust its selection throughout the generation process to maximize goal attainment, based on the current population of test suites. To test this hypothesis, we have implemented two reinforcement learning algorithms in the EvoSuite framework, and used these algorithms to dynamically set the fitness functions used during generation.We have evaluated our framework, EvoSuiteFIT, on a set of 386 real faults. EvoSuiteFIT discovers and retains more exception-triggering input and produces suites that detect a variety of faults missed by the other techniques. The ability to adjust fitness functions allows EvoSuiteFIT to make strategic choices that efficiently produce more effective test suites.
ER  - 

TY  - CONF
TI  - FinFuzzer: One Step Further in Fuzzing Fintech Systems
T2  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
SP  - 1111
EP  - 1115
AU  - Q. Wang
AU  - L. Xu
AU  - J. Xiao
AU  - Q. Guo
AU  - H. Zhang
AU  - L. Dou
AU  - L. He
AU  - T. Xie
PY  - 2021
DO  - 10.1109/ASE51524.2021.9678675
JO  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
IS  - 
SN  - 2643-1572
VO  - 
VL  - 
JA  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
Y1  - 15-19 Nov. 2021
AB  - Comprehensive testing is of high importance to ensure the reliability of software systems, especially for systems with high stakes such as FinTech systems. In this paper, we share our observations of the Ant Group’s status quo in testing their financial services, specifically on the importance of properly transforming relevant external environment settings and prioritizing input object fields for mutation during automated fuzzing. Based on these observations, we propose FinFuzzer, an automated fuzz testing framework that detects and transforms relevant environmental settings into system inputs, prioritizes input object fields, and mutates system inputs on both environment settings and high-priority object fields. Our evaluation of FinFuzzer against four FinTech systems developed by the Ant Group shows that FinFuzzer can outperform a state-of-the-art approach in terms of line coverage in much shorter time.
ER  - 

TY  - JOUR
TI  - A Seed Scheduling Method With a Reinforcement Learning for a Coverage Guided Fuzzing
T2  - IEEE Access
SP  - 2048
EP  - 2057
AU  - G. Choi
AU  - S. Jeon
AU  - J. Cho
AU  - J. Moon
PY  - 2023
DO  - 10.1109/ACCESS.2022.3233875
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - Seed scheduling, which determines which seed is input to the fuzzer first and the number of mutated test cases that are generated for the input seed, significantly influences crash detection performance in fuzz testing. Even for the same fuzzer, the performance in terms of detecting crashes that cause program failure varies considerably depending on the seed-scheduling method used. Most existing coverage-guided fuzzers use a heuristic seed-scheduling method. These heuristic methods can’t properly determine the seed with a high potential to cause the crash; thus, the fuzzer detects the crash inefficiently. Moreover, the fuzzer’s crash detection performance is affected by the characteristics of target programs. To address this problem, we propose a general-purpose reinforced seed-scheduling method that not only improves the crash detection performance of fuzz testing but also remains unaffected by the characteristics of the target program. The fuzzer with the proposed method detected the most crashes in all but one of the target programs in which crashes were detected in the experimental results conducted on various programs, and showed better crash detection efficiency than the comparison targets overall.
ER  - 

TY  - CONF
TI  - Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks
T2  - 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
SP  - 1571
EP  - 1582
AU  - V. Monjezi
AU  - A. Trivedi
AU  - G. Tan
AU  - S. Tizpaz-Niari
PY  - 2023
DO  - 10.1109/ICSE48619.2023.00136
JO  - 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
IS  - 
SN  - 1558-1225
VO  - 
VL  - 
JA  - 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
Y1  - 14-20 May 2023
AB  - The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding min-imal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions-amplifying existing biases or introducing new ones-that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids-such as severity and causal explanations-crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present Dice: an information-theoretic testing and debugging framework to discover and localize fairness defects in DNNs. The key goal of Dice is to assist software developers in triaging fairness defects by ordering them by their severity. Towards this goal, we quantify fairness in terms of protected information (in bits) used in decision making. A quantitative view of fairness defects not only helps in ordering these defects, our empirical evaluation shows that it improves the search efficiency due to resulting smoothness of the search space. Guided by the quan-titative fairness, we present a causal debugging framework to localize inadequately trained layers and neurons responsible for fairness defects. Our experiments over ten DNNs, developed for socially critical tasks, show that Dice efficiently characterizes the amounts of discrimination, effectively generates discriminatory instances (vis-a-vis the state-of-the-art techniques), and localizes layers/neurons with significant biases.
ER  - 

TY  - CONF
TI  - Algorithmic Input Generation for More Effective Software Testing
T2  - 2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)
SP  - 1708
EP  - 1715
AU  - L. Epifanovskaya
AU  - R. Meeson
AU  - C. McCormack
AU  - J. R. Lee
AU  - R. C. Armstrong
AU  - J. R. Mayo
PY  - 2022
DO  - 10.1109/COMPSAC54236.2022.00272
JO  - 2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)
IS  - 
SN  - 0730-3157
VO  - 
VL  - 
JA  - 2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)
Y1  - 27 June-1 July 2022
AB  - It is impossible in practice to comprehensively test even small software programs due to the vastness of the reachable state space; however, modern cyber-physical systems such as aircraft require a high degree of confidence in software safety and reliability. Here we explore methods of generating test sets to effectively and efficiently explore the state space for a module based on the Traffic Collision Avoidance System (TCAS) used on commercial aircraft. A formal model of TCAS in the model-checking language NuSMV provides an output oracle. We compare test sets generated using various methods, including covering arrays, random, and a low-complexity input paradigm applied to 28 versions of the TCAS C program containing seeded errors. Faults are triggered by tests for all 28 programs using a combination of covering arrays and random input generation. Complexity-based inputs perform more efficiently than covering arrays, and can be paired with random input generation to create efficient and effective test sets. A random forest classifier identifies variable values that can be targeted to generate tests even more efficiently in future work, by combining a machine-learned fuzzing algorithm with more complex model oracles developed in model-based systems engineering (MBSE) software.
ER  - 

TY  - CONF
TI  - SYSMODIS: A Systematic Model Discovery Approach
T2  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
SP  - 67
EP  - 76
AU  - O. Korkmaz
AU  - C. Yilmaz
PY  - 2021
DO  - 10.1109/ICSTW52544.2021.00023
JO  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
Y1  - 12-16 April 2021
AB  - In this paper, we present an automated model discovery approach, called SYSMODIS, which uses covering arrays to systematically sample the input spaces. SYSMODIS discovers finite state machine-based models, where states represent distinct screens and the edges between the states represent the transitions between the screens. SYSMODIS also discovers the likely guard conditions for the transitions, i.e., the conditions that must be satisfied before the transitions can be taken. For the first time a previously unseen screen is visited, a covering array-based test suite for the input fields present on the screen as well as the actions that can be taken on the screen, is created. SYSMODIS keeps on crawling until all the test suites for all the screens have been exhaustively tested. Once the crawling is over, the results of the test suites are fed to a machine learning algorithm on a per screen basis to determine the likely guard conditions. In the experiments we carried out to evaluate the proposed approach, we observed that SYSMODIS profoundly improved the state/screen coverage, transition coverage, and/or the accuracy of the predicted guard conditions, compared to the existing approaches studied in the paper.
ER  - 

TY  - CONF
TI  - Automatic Error Classification and Root Cause Determination while Replaying Recorded Workload Data at SAP HANA
T2  - 2022 IEEE Conference on Software Testing, Verification and Validation (ICST)
SP  - 323
EP  - 333
AU  - N. Jambigi
AU  - T. Bach
AU  - F. Schabernack
AU  - M. Felderer
PY  - 2022
DO  - 10.1109/ICST53961.2022.00041
JO  - 2022 IEEE Conference on Software Testing, Verification and Validation (ICST)
IS  - 
SN  - 2159-4848
VO  - 
VL  - 
JA  - 2022 IEEE Conference on Software Testing, Verification and Validation (ICST)
Y1  - 4-14 April 2022
AB  - Capturing customer workloads of database systems to replay these workloads during internal testing can be beneficial for software quality assurance. However, we experienced that such replays can produce a large amount of false positive alerts that make the results unreliable or time consuming to analyze. Therefore, we design a machine learning based approach that attributes root causes to the alerts. This provides several benefits for quality assurance and allows for example to classify whether an alert is true positive or false positive. Our approach considerably reduces manual effort and improves the overall quality assurance for the database system SAP HANA. We discuss the problem, the design and result of our approach, and we present practical limitations that may require further research.
ER  - 

TY  - CONF
TI  - DeepTLE: Learning Code-Level Features to Predict Code Performance before It Runs
T2  - 2019 26th Asia-Pacific Software Engineering Conference (APSEC)
SP  - 252
EP  - 259
AU  - M. Zhou
AU  - J. Chen
AU  - H. Hu
AU  - J. Yu
AU  - Z. Li
AU  - H. Hu
PY  - 2019
DO  - 10.1109/APSEC48747.2019.00042
JO  - 2019 26th Asia-Pacific Software Engineering Conference (APSEC)
IS  - 
SN  - 2640-0715
VO  - 
VL  - 
JA  - 2019 26th Asia-Pacific Software Engineering Conference (APSEC)
Y1  - 2-5 Dec. 2019
AB  - With the continuous expansion of the software market and the updating of the maturity of the software development process, the performance requirements of software users are becoming increasingly prominent. Performance issues are essentially related to the source code. For solving the same problem, different programmers may write completely different "correct" code with the same functionality but have different performance. Most online judge system on programming make use of automated grading systems, usually rely on test results to quantify the correctness and performance for the submitted source code. However, traditional dynamic testing takes a lot of time, and the discovery of performance problems is usually after the fact even for those small scale programs. Therefore, we proposed DeepTLE which is used to effectively predict the performance of submitted source code before it runs. DeepTLE can automatically learn the semantic and structural features of the source code. In order to verify the effect of our approach, we applied it to the source code collected from the program competition website to predict if the source code would be time limit exceed or not without running its test cases. Experiment results show that our method can save 96% of the time cost compared to the dynamic testing, and the accuracy of the prediction reaches 82%.
ER  - 

TY  - CONF
TI  - An Approach For Verifying And Validating Clustering Based Anomaly Detection Systems Using Metamorphic Testing
T2  - 2022 IEEE International Conference On Artificial Intelligence Testing (AITest)
SP  - 12
EP  - 18
AU  - F. U. Rehman
AU  - C. Izurieta
PY  - 2022
DO  - 10.1109/AITest55621.2022.00011
JO  - 2022 IEEE International Conference On Artificial Intelligence Testing (AITest)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE International Conference On Artificial Intelligence Testing (AITest)
Y1  - 15-18 Aug. 2022
AB  - An oracle or test oracle is a mechanism that a software tester uses to verify the program output. In software testing, the oracle problem arises when either the oracle is not available or it may be available but is so expensive that it is infeasible to apply. To help address this problem in testing machine learning-based applications, we propose an approach for testing clustering algorithms. We exemplify this in the implementation of the award-winning density-based clustering algorithm i.e., Density-based Spatial Clustering of Applications with Noise (DBSCAN). Our proposed approach is based on the ‘Metamorphic Testing’ technique which is considered an effective approach in alleviating the oracle problem. Our contributions in this paper include, i) proposing and showing the applicability of a broader set of 21 Metamorphic Relations (MRs), among which 8 target the verification aspect, whereas, 14 of them target the validation aspect of testing the algorithm under test, and ii) identifying and segregating the MRs (by providing a detailed analysis) to help both naive and expert users understand how the proposed MRs target both the verification and validation aspects of testing the DBSCAN algorithm. To show the effectiveness of the proposed approach, we further conduct a case study on an anomaly detection system. The results obtained show that, i) different MRs have the ability to reveal different violation rates (for the given data instances); thus, showing their effectiveness, and ii) although we have not found any implementation issues (through verification) in the algorithm under test (that further enhances our trust in the implementation), the results suggest that the DBSCAN algorithm may not be suitable for scenarios (meeting the user expectations a.k.a validation) captured by almost 79% of violated MRs; which show high susceptibility to small changes in the dataset.
ER  - 

TY  - JOUR
TI  - Open Science in Software Engineering: A Study on Deep Learning-Based Vulnerability Detection
T2  - IEEE Transactions on Software Engineering
SP  - 1983
EP  - 2005
AU  - Y. Nong
AU  - R. Sharma
AU  - A. Hamou-Lhadj
AU  - X. Luo
AU  - H. Cai
PY  - 2023
DO  - 10.1109/TSE.2022.3207149
JO  - IEEE Transactions on Software Engineering
IS  - 4
SN  - 1939-3520
VO  - 49
VL  - 49
JA  - IEEE Transactions on Software Engineering
Y1  - 1 April 2023
AB  - Open science is a practice that makes scientific research publicly accessible to anyone, hence is highly beneficial. Given the benefits, the software engineering (SE) community has been diligently advocating open science policies during peer reviews and publication processes. However, to this date, there has been few studies that look into the status and issues of open science in SE from a systematic perspective. In this paper, we set out to start filling this gap. Given the great breadth of SE in general, we constrained our scope to a particular topic area in SE as an example case. Recently, an increasing number of deep learning (DL) approaches have been explored in SE, including DL-based software vulnerability detection, a popular, fast-growing topic that addresses an important problem in software security. We exhaustively searched the literature in this area and identified 55 relevant works that propose a DL-based vulnerability detection approach. This was then followed by comprehensively investigating the four integral aspects of open science: availability, executability, reproducibility, and replicability. Among other findings, our study revealed that only a small percentage (25.5%) of the studied approaches provided publicly available tools. Some of these available tools did not provide sufficient documentation and complete implementation, making them not executable or not reproducible. The uses of balanced or artificially generated datasets caused significantly overrated performance of the respective techniques, making most of them not replicable. Based on our empirical results, we made actionable suggestions on improving the state of open science in each of the four aspects. We note that our results and recommendations on most of these aspects (availability, executability, reproducibility) are not tied to the nature of the chosen topic (DL-based vulnerability detection) hence are likely applicable to other SE topic areas. We also believe our results and recommendations on replicability to be applicable to other DL-based topics in SE as they are not tied to (the particular application of DL in) detecting software vulnerabilities.
ER  - 

TY  - CONF
TI  - On the Problem of Developing a Realistic Road Infrastructure Simulator for Reinforced Learning
T2  - 2023 Wave Electronics and its Application in Information and Telecommunication Systems (WECONF)
SP  - 1
EP  - 8
AU  - K. Polyantseva
AU  - M. Gorodnichev
AU  - Y. Mitrokhin
AU  - M. Moseva
PY  - 2023
DO  - 10.1109/WECONF57201.2023.10147983
JO  - 2023 Wave Electronics and its Application in Information and Telecommunication Systems (WECONF)
IS  - 
SN  - 2769-3538
VO  - 
VL  - 
JA  - 2023 Wave Electronics and its Application in Information and Telecommunication Systems (WECONF)
Y1  - 29 May-2 June 2023
AB  - One of the serious problems faced by urban municipalities is traffic congestion. It makes life in cities inconvenient for people. Every year, the government spends large budgets to solve this problem. Because of poorly planned road networks, a common result in many developing regions is small critical areas that are often hotspots of congestion; poor traffic management around these hotspots can lead to prolonged congestion. The work aims to develop a traffic flow modeling environment for the street and road network. This paper presents an analysis of already created solutions and tools for realistic urban simulation environments. The paper describes the stages of simulator design: requirements, methods of creation, functionality. The implementation of simulator components is described: development of the application and interaction subjects, software testing.
ER  - 

TY  - JOUR
TI  - To Be Optimal or Not in Test-Case Prioritization
T2  - IEEE Transactions on Software Engineering
SP  - 490
EP  - 505
AU  - D. Hao
AU  - L. Zhang
AU  - L. Zang
AU  - Y. Wang
AU  - X. Wu
AU  - T. Xie
PY  - 2016
DO  - 10.1109/TSE.2015.2496939
JO  - IEEE Transactions on Software Engineering
IS  - 5
SN  - 1939-3520
VO  - 42
VL  - 42
JA  - IEEE Transactions on Software Engineering
Y1  - 1 May 2016
AB  - Software testing aims to assure the quality of software under test. To improve the efficiency of software testing, especially regression testing, test-case prioritization is proposed to schedule the execution order of test cases in software testing. Among various test-case prioritization techniques, the simple additional coverage-based technique, which is a greedy strategy, achieves surprisingly competitive empirical results. To investigate how much difference there is between the order produced by the additional technique and the optimal order in terms of coverage, we conduct a study on various empirical properties of optimal coverage-based test-case prioritization. To enable us to achieve the optimal order in acceptable time for our object programs, we formulate optimal coverage-based test-case prioritization as an integer linear programming (ILP) problem. Then we conduct an empirical study for comparing the optimal technique with the simple additional coverage-based technique. From this empirical study, the optimal technique can only slightly outperform the additional coverage-based technique with no statistically significant difference in terms of coverage, and the latter significantly outperforms the former in terms of either fault detection or execution time. As the optimal technique schedules the execution order of test cases based on their structural coverage rather than detected faults, we further implement the ideal optimal test-case prioritization technique, which schedules the execution order of test cases based on their detected faults. Taking this ideal technique as the upper bound of test-case prioritization, we conduct another empirical study for comparing the optimal technique and the simple additional technique with this ideal technique. From this empirical study, both the optimal technique and the additional technique significantly outperform the ideal technique in terms of coverage, but the latter significantly outperforms the former two techniques in terms of fault detection. Our findings indicate that researchers may need take cautions in pursuing the optimal techniques in test-case prioritization with intermediate goals.
ER  - 

TY  - CONF
TI  - Application of Machine Learning for GUI Test Automation
T2  - 2022 XXVIII International Conference on Information, Communication and Automation Technologies (ICAT)
SP  - 1
EP  - 6
AU  - R. Walia
PY  - 2022
DO  - 10.1109/ICAT54566.2022.9811187
JO  - 2022 XXVIII International Conference on Information, Communication and Automation Technologies (ICAT)
IS  - 
SN  - 2643-1858
VO  - 
VL  - 
JA  - 2022 XXVIII International Conference on Information, Communication and Automation Technologies (ICAT)
Y1  - 16-18 June 2022
AB  - This paper examines the implementation of machine learning (ML) capabilities in a test automation suite, specifically for automation of graphical user interface (GUI) testing on an electronic design automation (EDA) tool within an integrated circuit (IC) physical design, verification, and implementation flow. We present a case study using existing tests to extract information and propose an ML implementation framework that consists of three modules, which can be adopted as a systematic pattern for test development. Our study focusses on implementation of the third module in this framework. We use the learnings from iterative testing patterns on a set of EDA tools provided by the Calibre RealTime interfaces from Siemens Digital Industries Software. The goal is to reduce human effort in selection and implementation of test cases and reallocate those resources to integral parts of the testing process like, approving and acting. We first establish metrics and variables, utilize VGG16 architecture for image classification and perform training on test data, and achieve an ML model based on accuracy and precision. Using this result, we present ML implementation as part of the script development process and analyze its impact. Based on our results, we conclude the third module of a framework for inclusion of ML in a regression testing suite for GUI test automation.
ER  - 

TY  - CONF
TI  - When and Why Test Generators for Deep Learning Produce Invalid Inputs: an Empirical Study
T2  - 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
SP  - 1161
EP  - 1173
AU  - V. Riccio
AU  - P. Tonella
PY  - 2023
DO  - 10.1109/ICSE48619.2023.00104
JO  - 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
IS  - 
SN  - 1558-1225
VO  - 
VL  - 
JA  - 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
Y1  - 14-20 May 2023
AB  - Testing Deep Learning (DL) based systems inherently requires large and representative test sets to evaluate whether DL systems generalise beyond their training datasets. Diverse Test Input Generators (TIGs) have been proposed to produce artificial inputs that expose issues of the DL systems by triggering misbehaviours. Unfortunately, such generated inputs may be invalid, i.e., not recognisable as part of the input domain, thus providing an unreliable quality assessment. Automated validators can ease the burden of manually checking the validity of inputs for human testers, although input validity is a concept difficult to formalise and, thus, automate. In this paper, we investigate to what extent TIGs can generate valid inputs, according to both automated and human validators. We conduct a large empirical study, involving 2 different automated validators, 220 human assessors, 5 different TIGs and 3 classification tasks. Our results show that 84% artificially generated inputs are valid, according to automated validators, but their expected label is not always preserved. Automated validators reach a good consensus with humans (78% accuracy), but still have limitations when dealing with feature-rich datasets.
ER  - 

TY  - CONF
TI  - Testing Service Oriented Architectures Using Stateful Service Virtualization via Machine Learning
T2  - 2018 IEEE/ACM 13th International Workshop on Automation of Software Test (AST)
SP  - 9
EP  - 15
AU  - H. F. Eniser
AU  - A. Sen
PY  - 2018
DO  - 
JO  - 2018 IEEE/ACM 13th International Workshop on Automation of Software Test (AST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE/ACM 13th International Workshop on Automation of Software Test (AST)
Y1  - 28-29 May 2018
AB  - Today's enterprise software systems are much complicated than the past. Increasing number of dependent applications, heterogeneous technologies and wide usage of Service Oriented Architectures (SOA), where numerous services communicate with each other, makes testing of such systems challenging. For testing these software systems, the concept of service virtualization is gaining popularity. Service virtualization is an automated technique to mimic the behavior of a given real service. Services can be classified as stateless or stateful services. Many services are stateful in nature. Although there are works in the literature for virtualization of stateless services, no such solution exists for stateful services. To the best of our knowledge, this is the first work for stateful service virtualization. We employ classification based and sequence-to-sequence based machine learning algorithms in developing our solutions. We demonstrate the validity of our approach on two data sets collected from real life services and obtain promising results.
ER  - 

TY  - JOUR
TI  - Simulated Software Testing Process and Its Optimization Considering Heterogeneous Debuggers and Release Time
T2  - IEEE Access
SP  - 38649
EP  - 38659
AU  - K. Gao
PY  - 2021
DO  - 10.1109/ACCESS.2021.3064296
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 9
VL  - 9
JA  - IEEE Access
Y1  - 2021
AB  - Most traditional software reliability growth models (SRGMs) assume immediate fault correction upon detection and therefore only consider fault detection process (FDP). In order to be more realistic, some researchers have tried to incorporate fault correction process (FCP) and fault introduction process (FIP) into the software reliability models. However, it is still difficult to incorporate into the analytical software reliability models some other factors, such as the different fault detection and correction capabilities of debuggers. In this paper, a simulation approach is proposed to model FDP, FIP, and FCP together considering debuggers with different contributions to fault detection rate, different fault correction rate and different fault introduction rate. Besides, this paper also constructed a cost calculation method to optimize the testing design including debuggers assignment and software release time. Some numerical examples are provided to illustrate the proposed model. The results show that the trends of FDP, FCP and FIP are consistent with the intuition to the practice of software testing, and the optimal testing resources allocation and the optimal release time can be obtained according to the proposed model.
ER  - 

